/**
 * Simplified Test Server for GPU Chat Demo
 * Bypasses database issues for WebSocket testing
 */

import express from 'express';
import cors from 'cors';
import { WebSocketServer } from 'ws';
import { createServer } from 'http';
import dotenv from 'dotenv';

dotenv.config();

const app = express();
const server = createServer(app);
const wss = new WebSocketServer({ server });

const CONFIG = {
  PORT: process.env.GPU_ORCHESTRATOR_PORT || 4002,
  OLLAMA_ENDPOINT: process.env.OLLAMA_ENDPOINT || 'http://localhost:11434'
};

// Middleware
app.use(cors({
  origin: ['http://localhost:5173', 'http://localhost:5174', 'http://localhost:3000'],
  credentials: true
}));
app.use(express.json());

// Mock database responses
const MOCK_LEGAL_CONTEXT = [
  {
    id: 1,
    title: 'Legal Contract Fundamentals',
    summary: 'A legal contract is a binding agreement between two or more parties that creates mutual obligations enforceable by law.',
    distance: 0.2,
    type: 'contract',
    metadata: {}
  },
  {
    id: 2,
    title: 'Employment Law Basics',
    summary: 'Employment law governs the relationship between employers and employees, including hiring, workplace conditions, and termination.',
    distance: 0.3,
    type: 'employment',
    metadata: {}
  }
];

// Simple AI response without Ollama
async function generateMockResponse(message, context) {
  // Simulate processing time
  await new Promise(resolve => setTimeout(resolve, 2000));
  
  return {
    response: `Thank you for your legal question: "${message}"\n\nBased on the relevant legal context I found:\n\n${context.map(doc => `• ${doc.title}: ${doc.summary}`).join('\n')}\n\nThis is a mock response demonstrating the GPU-accelerated legal AI chat system. In a production environment, this would be generated by Gemma3-Legal model with comprehensive legal analysis.`,
    model: 'mock-gemma3-legal',
    created_at: new Date().toISOString(),
    done: true,
    total_duration: 2000000000, // 2 seconds in nanoseconds
    eval_count: 150
  };
}

// Health endpoint
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    activeRequests: 0,
    gpuEnabled: false,
    uptime: process.uptime(),
    services: {
      redis: true,
      database: true,
      ollama: false, // Mock mode
      cuda: 'disabled'
    }
  });
});

// Chat endpoint
app.post('/chat', async (req, res) => {
  try {
    const { message, userId, sessionId } = req.body;
    
    if (!message?.trim()) {
      return res.status(400).json({ error: 'Message is required' });
    }

    console.log(`🔹 Processing: "${message.substring(0, 50)}..."`);
    
    // Mock embedding generation
    console.log('🔹 Generating embedding (mock)...');
    await new Promise(resolve => setTimeout(resolve, 200));
    
    // Mock context retrieval
    console.log('🔹 Retrieving legal context (mock)...');
    const context = MOCK_LEGAL_CONTEXT;
    
    // Generate mock response
    console.log('🔹 Generating AI response (mock)...');
    const response = await generateMockResponse(message.trim(), context);
    
    console.log('✅ Chat processing complete');
    
    res.json({
      success: true,
      response: response.response,
      audio: null,
      context: context.length,
      metadata: {
        model: response.model,
        processingTime: response.total_duration,
        gpuAccelerated: false,
        tokens: response.eval_count
      },
      sessionId,
      userId,
      timestamp: Date.now()
    });
    
  } catch (error) {
    console.error('Chat endpoint error:', error);
    res.status(500).json({ 
      error: 'Internal server error',
      details: error.message 
    });
  }
});

// WebSocket handling
wss.on('connection', (ws) => {
  console.log('🔗 New WebSocket connection established');
  
  ws.on('message', async (data) => {
    try {
      const message = JSON.parse(data.toString());
      console.log('📨 Received WebSocket message:', message.type);
      
      if (message.type === 'chat') {
        console.log(`🔹 Processing WebSocket chat: "${message.content.substring(0, 30)}..."`);
        
        // Mock processing
        await new Promise(resolve => setTimeout(resolve, 100));
        console.log('🔹 Generating embedding (mock)...');
        
        await new Promise(resolve => setTimeout(resolve, 100));
        console.log('🔹 Retrieving legal context (mock)...');
        
        await new Promise(resolve => setTimeout(resolve, 2000));
        console.log('🔹 Generating AI response (mock)...');
        
        const response = await generateMockResponse(message.content, MOCK_LEGAL_CONTEXT);
        
        ws.send(JSON.stringify({
          type: 'response',
          id: message.id,
          success: true,
          response: response.response,
          audio: null,
          context: MOCK_LEGAL_CONTEXT.length,
          metadata: {
            model: response.model,
            processingTime: response.total_duration,
            gpuAccelerated: false,
            tokens: response.eval_count
          },
          timestamp: Date.now()
        }));
        
        console.log('✅ WebSocket chat processing complete');
      }
    } catch (error) {
      console.error('WebSocket message error:', error);
      ws.send(JSON.stringify({ 
        type: 'error',
        error: 'Invalid message format or processing error',
        details: error.message
      }));
    }
  });
  
  ws.on('close', () => {
    console.log('🔌 WebSocket connection closed');
  });
  
  ws.on('error', (error) => {
    console.error('WebSocket error:', error);
  });
});

// Start server
server.listen(CONFIG.PORT, () => {
  console.log(`
🎯 Test GPU-Accelerated Legal AI Running!
📡 Server: http://localhost:${CONFIG.PORT}
🔌 WebSocket: ws://localhost:${CONFIG.PORT}
🖥️  GPU Acceleration: ❌ Disabled (Test Mode)
🧠 AI Models: Mock Responses
💾 Database: Mock Data
⚡ Redis: Simulated

🧪 TEST MODE - Ready for WebSocket chat testing! 🚀
  `);
});

// Graceful shutdown
process.on('SIGTERM', () => {
  console.log('🛑 Shutting down test server...');
  server.close();
  process.exit(0);
});