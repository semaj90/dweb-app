// YoRHa Legal AI - Advanced GPU Neural Processor
// CUDA/cuBLAS Enhanced Document Processing System
// Version 3.0 - Neural Network Optimized

//go:build cuda && cublas
// +build cuda,cublas

package main

/*
#cgo CFLAGS: -I${CUDA_PATH}/include
#cgo LDFLAGS: -L${CUDA_PATH}/lib/x64 -lcudart -lcublas -lcurand -lcusolver

#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <curand.h>
#include <cusolver.h>
#include <stdio.h>
#include <stdlib.h>

// YoRHa CUDA initialization and error checking
int yorha_init_cuda() {
    cudaError_t err = cudaSetDevice(0);
    if (err != cudaSuccess) {
        printf("[YoRHa] CUDA device initialization failed: %s\n", cudaGetErrorString(err));
        return 0;
    }
    
    // Get device properties
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, 0);
    printf("[YoRHa] CUDA Device: %s\n", prop.name);
    printf("[YoRHa] CUDA Compute Capability: %d.%d\n", prop.major, prop.minor);
    printf("[YoRHa] CUDA Global Memory: %.2f GB\n", (float)prop.totalGlobalMem / (1024*1024*1024));
    
    return 1;
}

// YoRHa cuBLAS initialization
int yorha_init_cublas() {
    cublasHandle_t handle;
    cublasStatus_t status = cublasCreate(&handle);
    if (status != CUBLAS_STATUS_SUCCESS) {
        printf("[YoRHa] cuBLAS initialization failed: %d\n", status);
        return 0;
    }
    
    // Store handle globally for reuse
    printf("[YoRHa] cuBLAS initialized successfully\n");
    cublasDestroy(handle);
    return 1;
}

// YoRHa neural matrix operations using cuBLAS
int yorha_neural_matrix_multiply(float* a, float* b, float* c, int n) {
    cublasHandle_t handle;
    cublasStatus_t status = cublasCreate(&handle);
    if (status != CUBLAS_STATUS_SUCCESS) return 0;
    
    float *d_a, *d_b, *d_c;
    size_t size = n * n * sizeof(float);
    
    // Allocate GPU memory
    cudaMalloc(&d_a, size);
    cudaMalloc(&d_b, size);
    cudaMalloc(&d_c, size);
    
    // Copy data to GPU
    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);
    
    // Perform matrix multiplication using cuBLAS
    const float alpha = 1.0f, beta = 0.0f;
    status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, n, n, n, &alpha, d_a, n, d_b, n, &beta, d_c, n);
    
    // Copy result back
    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);
    
    // Cleanup
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
    cublasDestroy(handle);
    
    return status == CUBLAS_STATUS_SUCCESS ? 1 : 0;
}

// YoRHa GPU memory management
float yorha_get_gpu_memory_usage() {
    size_t free_mem, total_mem;
    cudaMemGetInfo(&free_mem, &total_mem);
    return ((float)(total_mem - free_mem) / total_mem) * 100.0f;
}

// YoRHa GPU temperature monitoring
int yorha_get_gpu_temperature() {
    // Note: This requires NVML (NVIDIA Management Library)
    // For now, return a simulated temperature
    return 65; // Simulated temperature in Celsius
}

// YoRHa neural document processing acceleration
int yorha_process_document_gpu(char* document, int length, float* confidence) {
    // Simulate advanced GPU-accelerated document processing
    // In a real implementation, this would use CUDA kernels for:
    // - OCR acceleration
    // - NLP processing
    // - Feature extraction
    // - Classification
    
    curandGenerator_t gen;
    curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT);
    curandSetPseudoRandomGeneratorSeed(gen, 1234ULL);
    
    float result;
    curandGenerateUniform(gen, &result, 1);
    *confidence = 0.85f + (result * 0.14f); // 85-99% confidence
    
    curandDestroyGenerator(gen);
    return 1;
}
*/
import "C"

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"os"
	"runtime"
	"sync"
	"time"
	"unsafe"

	"github.com/gin-gonic/gin"
)

// YoRHa Neural Processor with advanced GPU capabilities
type YoRHaNeuralProcessor struct {
	Mode                    string    `json:"mode"`
	Version                 string    `json:"version"`
	GPUEnabled              bool      `json:"gpu_enabled"`
	CUDAEnabled             bool      `json:"cuda_enabled"`
	CuBLASEnabled           bool      `json:"cublas_enabled"`
	StartTime               time.Time `json:"start_time"`
	ProcessedDocuments      int64     `json:"processed_documents"`
	GPUMemoryUsage          float32   `json:"gpu_memory_usage_percent"`
	GPUTemperature          int       `json:"gpu_temperature_celsius"`
	SystemStatus            string    `json:"system_status"`
	AccelerationMode        string    `json:"acceleration_mode"`
	NeuralNetworkVersion    string    `json:"neural_network_version"`
	TensorRTEnabled         bool      `json:"tensorrt_enabled"`
	CUDNNEnabled            bool      `json:"cudnn_enabled"`
	ProcessingCapabilities  []string  `json:"processing_capabilities"`
	mutex                   sync.RWMutex
}

// YoRHa Neural Processing Statistics
type YoRHaNeuralStats struct {
	TotalProcessingTime     time.Duration `json:"total_processing_time"`
	AverageProcessingTime   time.Duration `json:"average_processing_time"`
	SuccessfulProcesses     int64         `json:"successful_processes"`
	FailedProcesses         int64         `json:"failed_processes"`
	GPUAcceleratedProcesses int64         `json:"gpu_accelerated_processes"`
	NeuralConfidenceAverage float32       `json:"neural_confidence_average"`
}

// Global YoRHa processor instance
var (
	yorhaProcessor *YoRHaNeuralProcessor
	neuralStats    *YoRHaNeuralStats
	processingPool = make(chan struct{}, 8) // Limit concurrent processing
)

func init() {
	log.SetFlags(log.LstdFlags | log.Lshortfile)
	
	// Initialize YoRHa Neural Processor
	yorhaProcessor = &YoRHaNeuralProcessor{
		Mode:                   "YoRHa_Neural_GPU",
		Version:                "3.0.0-CUDA-Enhanced",
		GPUEnabled:             false,
		CUDAEnabled:            false,
		CuBLASEnabled:          false,
		StartTime:              time.Now(),
		SystemStatus:           "INITIALIZING",
		AccelerationMode:       "CPU_FALLBACK",
		NeuralNetworkVersion:   "YoRHa-Neural-v3.0",
		TensorRTEnabled:        false,
		CUDNNEnabled:           false,
		ProcessingCapabilities: []string{"CPU_Processing", "Memory_Optimization"},
	}
	
	neuralStats = &YoRHaNeuralStats{}
	
	// Initialize YoRHa CUDA systems
	initYoRHaCUDA()
}

func initYoRHaCUDA() {
	fmt.Println("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
	fmt.Println("ü§ñ YoRHa Legal AI Neural Processor - GPU Enhanced")
	fmt.Println("Advanced CUDA/cuBLAS Document Processing System")
	fmt.Println("Version 3.0 - Neural Network Optimized")
	fmt.Println("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
	
	// Initialize CUDA
	if int(C.yorha_init_cuda()) == 1 {
		yorhaProcessor.CUDAEnabled = true
		yorhaProcessor.GPUEnabled = true
		yorhaProcessor.ProcessingCapabilities = append(yorhaProcessor.ProcessingCapabilities, "CUDA_Acceleration", "GPU_Memory_Management")
		fmt.Println("üöÄ YoRHa CUDA neural acceleration initialized successfully")
		
		// Initialize cuBLAS
		if int(C.yorha_init_cublas()) == 1 {
			yorhaProcessor.CuBLASEnabled = true
			yorhaProcessor.AccelerationMode = "CUDA_CUBLAS_ENHANCED"
			yorhaProcessor.ProcessingCapabilities = append(yorhaProcessor.ProcessingCapabilities, "cuBLAS_Linear_Algebra", "Neural_Matrix_Operations")
			fmt.Println("‚ö° YoRHa cuBLAS neural acceleration enabled")
		} else {
			yorhaProcessor.AccelerationMode = "CUDA_BASIC"
			fmt.Println("‚ö° YoRHa basic CUDA neural acceleration enabled")
		}
		
		// Check for additional capabilities
		yorhaProcessor.TensorRTEnabled = checkTensorRT()
		yorhaProcessor.CUDNNEnabled = checkCUDNN()
		
		if yorhaProcessor.TensorRTEnabled {
			yorhaProcessor.ProcessingCapabilities = append(yorhaProcessor.ProcessingCapabilities, "TensorRT_Optimization")
			fmt.Println("üß† YoRHa TensorRT neural optimization available")
		}
		
		if yorhaProcessor.CUDNNEnabled {
			yorhaProcessor.ProcessingCapabilities = append(yorhaProcessor.ProcessingCapabilities, "cuDNN_Deep_Learning")
			fmt.Println("üß† YoRHa cuDNN deep learning acceleration available")
		}
		
	} else {
		fmt.Println("‚ö†Ô∏è  YoRHa CUDA initialization failed - using optimized CPU mode")
		yorhaProcessor.AccelerationMode = "CPU_NEURAL_OPTIMIZED"
		yorhaProcessor.ProcessingCapabilities = append(yorhaProcessor.ProcessingCapabilities, "CPU_SIMD_Optimization", "Multi_Threading")
	}
	
	yorhaProcessor.SystemStatus = "OPERATIONAL"
	
	fmt.Printf("üéØ YoRHa Acceleration Mode: %s\n", yorhaProcessor.AccelerationMode)
	fmt.Printf("üöÄ CUDA Enabled: %t\n", yorhaProcessor.CUDAEnabled)
	fmt.Printf("‚ö° cuBLAS Enabled: %t\n", yorhaProcessor.CuBLASEnabled)
	fmt.Printf("üß† TensorRT Enabled: %t\n", yorhaProcessor.TensorRTEnabled)
	fmt.Printf("üß† cuDNN Enabled: %t\n", yorhaProcessor.CUDNNEnabled)
	fmt.Println("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
}

func checkTensorRT() bool {
	// In a real implementation, check for TensorRT libraries
	return false // Simulated for now
}

func checkCUDNN() bool {
	// In a real implementation, check for cuDNN libraries
	return false // Simulated for now
}

func main() {
	// Set Gin to release mode for production
	gin.SetMode(gin.ReleaseMode)
	
	// Create YoRHa neural router
	router := gin.New()
	
	// YoRHa custom middleware
	router.Use(yorhaLoggingMiddleware())
	router.Use(gin.Recovery())
	
	// YoRHa Neural API endpoints
	setupYoRHaRoutes(router)
	
	// Start background monitoring
	go monitorYoRHaSystems()
	
	// Get API port from environment
	port := os.Getenv("API_PORT")
	if port == "" {
		port = "8080"
	}
	
	fmt.Printf("ü§ñ YoRHa Legal AI Neural Processor listening on port %s\n", port)
	fmt.Printf("üåê Access YoRHa neural interface at: http://localhost:%s\n", port)
	fmt.Printf("üìä YoRHa system metrics at: http://localhost:%s/metrics\n", port)
	fmt.Printf("üéÆ YoRHa GPU information at: http://localhost:%s/gpu-info\n", port)
	
	log.Fatal(router.Run(":" + port))
}

func setupYoRHaRoutes(router *gin.Engine) {
	// YoRHa system health endpoint
	router.GET("/health", func(c *gin.Context) {
		yorhaProcessor.mutex.RLock()
		defer yorhaProcessor.mutex.RUnlock()
		
		// Update GPU metrics if CUDA is enabled
		if yorhaProcessor.CUDAEnabled {
			yorhaProcessor.GPUMemoryUsage = float32(C.yorha_get_gpu_memory_usage())
			yorhaProcessor.GPUTemperature = int(C.yorha_get_gpu_temperature())
		}
		
		c.JSON(http.StatusOK, gin.H{
			"status":               "YoRHa Legal AI Neural System - OPERATIONAL",
			"neural_unit":          "2B-9S-A2",
			"mode":                 yorhaProcessor.Mode,
			"version":              yorhaProcessor.Version,
			"neural_network":       yorhaProcessor.NeuralNetworkVersion,
			"acceleration":         yorhaProcessor.AccelerationMode,
			"cuda_enabled":         yorhaProcessor.CUDAEnabled,
			"cublas_enabled":       yorhaProcessor.CuBLASEnabled,
			"tensorrt_enabled":     yorhaProcessor.TensorRTEnabled,
			"cudnn_enabled":        yorhaProcessor.CUDNNEnabled,
			"uptime":               time.Since(yorhaProcessor.StartTime).String(),
			"memory_cpu":           fmt.Sprintf("%.2f MB", float64(getMemUsage())/1024/1024),
			"memory_gpu_usage":     fmt.Sprintf("%.1f%%", yorhaProcessor.GPUMemoryUsage),
			"gpu_temperature":      fmt.Sprintf("%d¬∞C", yorhaProcessor.GPUTemperature),
			"goroutines":           runtime.NumGoroutine(),
			"processed_documents":  yorhaProcessor.ProcessedDocuments,
			"processing_capabilities": yorhaProcessor.ProcessingCapabilities,
			"timestamp":            time.Now().Format(time.RFC3339),
		})
	})
	
	// YoRHa detailed metrics endpoint
	router.GET("/metrics", func(c *gin.Context) {
		yorhaProcessor.mutex.RLock()
		neuralStats := *neuralStats
		processor := *yorhaProcessor
		yorhaProcessor.mutex.RUnlock()
		
		c.JSON(http.StatusOK, gin.H{
			"yorha_processor": processor,
			"neural_statistics": neuralStats,
			"system_info": gin.H{
				"go_version":    runtime.Version(),
				"architecture":  runtime.GOARCH,
				"operating_system": runtime.GOOS,
				"cpu_cores":     runtime.NumCPU(),
			},
		})
	})
	
	// YoRHa GPU information endpoint
	router.GET("/gpu-info", func(c *gin.Context) {
		c.JSON(http.StatusOK, gin.H{
			"gpu_available":          yorhaProcessor.GPUEnabled,
			"cuda_support":           yorhaProcessor.CUDAEnabled,
			"cublas_support":         yorhaProcessor.CuBLASEnabled,
			"tensorrt_support":       yorhaProcessor.TensorRTEnabled,
			"cudnn_support":          yorhaProcessor.CUDNNEnabled,
			"acceleration_mode":      yorhaProcessor.AccelerationMode,
			"neural_network_version": yorhaProcessor.NeuralNetworkVersion,
			"gpu_memory_usage":       yorhaProcessor.GPUMemoryUsage,
			"gpu_temperature":        yorhaProcessor.GPUTemperature,
			"processing_capabilities": yorhaProcessor.ProcessingCapabilities,
		})
	})
	
	// YoRHa neural document processing endpoint
	router.POST("/process", func(c *gin.Context) {
		// Use processing pool to limit concurrent operations
		processingPool <- struct{}{}
		defer func() { <-processingPool }()
		
		start := time.Now()
		
		var requestData map[string]interface{}
		if err := c.ShouldBindJSON(&requestData); err != nil {
			c.JSON(http.StatusBadRequest, gin.H{
				"error": "Invalid YoRHa neural request format",
				"details": err.Error(),
			})
			return
		}
		
		// Extract document content
		documentContent, exists := requestData["document"].(string)
		if !exists {
			documentContent = "sample_yorha_document"
		}
		
		// Process document using YoRHa neural systems
		var confidence float32
		success := processDocumentYoRHa(documentContent, &confidence)
		
		processingTime := time.Since(start)
		
		// Update statistics
		yorhaProcessor.mutex.Lock()
		yorhaProcessor.ProcessedDocuments++
		neuralStats.TotalProcessingTime += processingTime
		
		if success {
			neuralStats.SuccessfulProcesses++
			if yorhaProcessor.CUDAEnabled {
				neuralStats.GPUAcceleratedProcesses++
			}
		} else {
			neuralStats.FailedProcesses++
		}
		
		// Update average confidence
		if neuralStats.SuccessfulProcesses > 0 {
			neuralStats.NeuralConfidenceAverage = (neuralStats.NeuralConfidenceAverage*float32(neuralStats.SuccessfulProcesses-1) + confidence) / float32(neuralStats.SuccessfulProcesses)
		}
		
		// Update average processing time
		totalProcesses := neuralStats.SuccessfulProcesses + neuralStats.FailedProcesses
		if totalProcesses > 0 {
			neuralStats.AverageProcessingTime = neuralStats.TotalProcessingTime / time.Duration(totalProcesses)
		}
		yorhaProcessor.mutex.Unlock()
		
		// Prepare response
		response := gin.H{
			"status":                "Document processed by YoRHa neural system",
			"neural_unit":           "2B-9S-A2",
			"processing_time":       processingTime.String(),
			"acceleration_mode":     yorhaProcessor.AccelerationMode,
			"neural_confidence":     fmt.Sprintf("%.2f%%", confidence*100),
			"processed_count":       yorhaProcessor.ProcessedDocuments,
			"gpu_accelerated":       yorhaProcessor.CUDAEnabled,
			"success":               success,
			"timestamp":             time.Now().Format(time.RFC3339),
		}
		
		if success {
			c.JSON(http.StatusOK, response)
		} else {
			response["error"] = "YoRHa neural processing failed"
			c.JSON(http.StatusInternalServerError, response)
		}
	})
	
	// YoRHa neural batch processing endpoint
	router.POST("/process-batch", func(c *gin.Context) {
		var batchRequest struct {
			Documents []string `json:"documents"`
			BatchSize int      `json:"batch_size,omitempty"`
		}
		
		if err := c.ShouldBindJSON(&batchRequest); err != nil {
			c.JSON(http.StatusBadRequest, gin.H{
				"error": "Invalid YoRHa neural batch request",
				"details": err.Error(),
			})
			return
		}
		
		if len(batchRequest.Documents) == 0 {
			c.JSON(http.StatusBadRequest, gin.H{
				"error": "No documents provided for YoRHa neural batch processing",
			})
			return
		}
		
		batchSize := batchRequest.BatchSize
		if batchSize <= 0 || batchSize > 32 {
			batchSize = 8 // Default YoRHa batch size
		}
		
		start := time.Now()
		results := processBatchYoRHa(batchRequest.Documents, batchSize)
		processingTime := time.Since(start)
		
		c.JSON(http.StatusOK, gin.H{
			"status":            "YoRHa neural batch processing complete",
			"neural_unit":       "2B-9S-A2",
			"total_documents":   len(batchRequest.Documents),
			"batch_size":        batchSize,
			"processing_time":   processingTime.String(),
			"acceleration_mode": yorhaProcessor.AccelerationMode,
			"results":           results,
			"timestamp":         time.Now().Format(time.RFC3339),
		})
	})
	
	// YoRHa system shutdown endpoint
	router.POST("/shutdown", func(c *gin.Context) {
		go func() {
			time.Sleep(2 * time.Second)
			fmt.Println("ü§ñ YoRHa Legal AI Neural Processor shutting down gracefully...")
			os.Exit(0)
		}()
		
		c.JSON(http.StatusOK, gin.H{
			"status": "YoRHa neural system shutdown initiated",
			"neural_unit": "2B-9S-A2",
			"message": "All YoRHa neural processes will terminate in 2 seconds",
		})
	})
}

func processDocumentYoRHa(document string, confidence *float32) bool {
	if yorhaProcessor.CUDAEnabled {
		// Use CUDA acceleration for document processing
		cDocument := C.CString(document)
		defer C.free(unsafe.Pointer(cDocument))
		
		var cConfidence C.float
		result := int(C.yorha_process_document_gpu(cDocument, C.int(len(document)), &cConfidence))
		*confidence = float32(cConfidence)
		
		return result == 1
	} else {
		// CPU-based neural processing simulation
		time.Sleep(time.Duration(50+len(document)/100) * time.Millisecond) // Simulate processing time
		
		// Simulate neural confidence calculation
		*confidence = 0.75 + (float32(len(document)%25) / 100.0) // 75-100% confidence range
		
		return true
	}
}

func processBatchYoRHa(documents []string, batchSize int) []map[string]interface{} {
	results := make([]map[string]interface{}, len(documents))
	
	// Process documents in parallel batches
	sem := make(chan struct{}, batchSize)
	var wg sync.WaitGroup
	
	for i, doc := range documents {
		wg.Add(1)
		go func(index int, document string) {
			defer wg.Done()
			sem <- struct{}{} // Acquire semaphore
			defer func() { <-sem }() // Release semaphore
			
			start := time.Now()
			var confidence float32
			success := processDocumentYoRHa(document, &confidence)
			processingTime := time.Since(start)
			
			results[index] = map[string]interface{}{
				"document_id":      index + 1,
				"success":          success,
				"confidence":       confidence,
				"processing_time":  processingTime.String(),
				"gpu_accelerated":  yorhaProcessor.CUDAEnabled,
			}
		}(i, doc)
	}
	
	wg.Wait()
	return results
}

func monitorYoRHaSystems() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			if yorhaProcessor.CUDAEnabled {
				// Update GPU metrics
				yorhaProcessor.mutex.Lock()
				yorhaProcessor.GPUMemoryUsage = float32(C.yorha_get_gpu_memory_usage())
				yorhaProcessor.GPUTemperature = int(C.yorha_get_gpu_temperature())
				yorhaProcessor.mutex.Unlock()
				
				// Check for thermal throttling
				if yorhaProcessor.GPUTemperature > 85 {
					log.Printf("[YoRHa] WARNING: GPU temperature high: %d¬∞C", yorhaProcessor.GPUTemperature)
				}
				
				// Check for memory pressure
				if yorhaProcessor.GPUMemoryUsage > 90 {
					log.Printf("[YoRHa] WARNING: GPU memory usage high: %.1f%%", yorhaProcessor.GPUMemoryUsage)
				}
			}
		}
	}
}

func yorhaLoggingMiddleware() gin.HandlerFunc {
	return gin.LoggerWithFormatter(func(param gin.LogFormatterParams) string {
		return fmt.Sprintf("[YoRHa] %v |%s %3d %s| %13v | %15s |%s %-7s %s %#v\n%s",
			param.TimeStamp.Format("2006/01/02 - 15:04:05"),
			param.StatusCodeColor(),
			param.StatusCode,
			param.ResetColor(),
			param.Latency,
			param.ClientIP,
			param.MethodColor(),
			param.Method,
			param.ResetColor(),
			param.Path,
			param.ErrorMessage,
		)
	})
}

func getMemUsage() uint64 {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return m.Alloc
}
