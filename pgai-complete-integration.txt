# ==========================================
# COMPLETE PGAI + LOCAL GEMMA3 GGUF + RAG INTEGRATION
# Windows Native - No Docker - Best Practices
# ==========================================

param(
    [string]$InstallPath = "C:\legal-ai-system",
    [string]$PostgresPath = "C:\Program Files\PostgreSQL\17",
    [string]$GGUFPath = "C:\Users\james\Desktop\deeds-web\deeds-web-app\gemma3Q4_K_M\mohf16-Q4_K_M.gguf"
)

Write-Host "ðŸš€ Complete pgai + Local GGUF + Enhanced RAG Integration" -ForegroundColor Cyan
Write-Host "=======================================================" -ForegroundColor Cyan
Write-Host "Using local model: $GGUFPath" -ForegroundColor Yellow

# ==========================================
# 1. INSTALL PGAI EXTENSION FOR POSTGRESQL 17
# ==========================================

Write-Host "`nðŸ“¦ Installing pgai extension for PostgreSQL 17..." -ForegroundColor Yellow

# Check if PostgreSQL 17 is installed
if (!(Test-Path "$PostgresPath\bin\psql.exe")) {
    Write-Error "PostgreSQL 17 not found at $PostgresPath"
    exit 1
}

# Clone and build pgai
$pgaiPath = "$env:TEMP\pgai"
if (!(Test-Path $pgaiPath)) {
    git clone https://github.com/timescale/pgai.git $pgaiPath
}

Push-Location $pgaiPath

# Build pgai for Windows
Write-Host "Building pgai for PostgreSQL 17..." -ForegroundColor Yellow
$env:PATH = "$PostgresPath\bin;$env:PATH"

# Build with MSVC for Windows
if (Get-Command nmake -ErrorAction SilentlyContinue) {
    & "$PostgresPath\bin\pg_config.exe" | Out-Null
    nmake /f Makefile.win
    nmake /f Makefile.win install
} else {
    Write-Warning "MSVC not found, attempting MinGW build..."
    make USE_PGXS=1 PG_CONFIG="$PostgresPath\bin\pg_config.exe"
    make USE_PGXS=1 PG_CONFIG="$PostgresPath\bin\pg_config.exe" install
}

Pop-Location

Write-Host "âœ… pgai extension installed" -ForegroundColor Green

# ==========================================
# 2. SETUP LOCAL GEMMA3 GGUF MODEL WITH OLLAMA
# ==========================================

Write-Host "`nðŸ¤– Setting up local Gemma3 GGUF model..." -ForegroundColor Yellow

# Ensure Ollama is running
$ollamaProcess = Get-Process "ollama" -ErrorAction SilentlyContinue
if (!$ollamaProcess) {
    Write-Host "Starting Ollama service..." -ForegroundColor Yellow
    Start-Process -FilePath "ollama" -ArgumentList "serve" -WindowStyle Hidden
    Start-Sleep -Seconds 5
}

# Create Modelfile for local GGUF
$modelfilePath = "$InstallPath\Modelfile-gemma3-legal"
@"
FROM $GGUFPath

TEMPLATE """<bos><start_of_turn>user
{{ if .System }}{{ .System }}

{{ end }}{{ .Prompt }}<end_of_turn>
<start_of_turn>model
{{ .Response }}<end_of_turn>"""

SYSTEM """You are a specialized Legal AI Assistant powered by Gemma 3. You provide:
1. Document summarization and key point extraction
2. Semantic embeddings for RAG retrieval
3. Legal recommendations and insights
4. Evidence analysis and correlation
Always maintain professional standards and cite relevant legal precedents when available."""

PARAMETER temperature 0.2
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.05
PARAMETER num_ctx 8192
PARAMETER num_gpu 999
PARAMETER num_thread 8
"@ | Out-File -FilePath $modelfilePath -Encoding UTF8

# Create the model in Ollama
Write-Host "Creating Ollama model from local GGUF..." -ForegroundColor Yellow
& ollama create gemma3-legal -f $modelfilePath

# Also create specialized models for different tasks
Write-Host "Creating specialized models..." -ForegroundColor Yellow

# Summarization model (lower temperature for consistency)
$summaryModelfile = "$InstallPath\Modelfile-gemma3-summary"
@"
FROM $GGUFPath

SYSTEM """You are a legal document summarizer. Extract and summarize:
1. Key facts and findings
2. Legal implications
3. Important dates and deadlines
4. Parties involved
5. Action items
Format output as structured JSON."""

PARAMETER temperature 0.1
PARAMETER top_p 0.8
PARAMETER num_ctx 4096
"@ | Out-File -FilePath $summaryModelfile -Encoding UTF8
& ollama create gemma3-summary -f $summaryModelfile

# Embedding model (deterministic for consistency)
$embedModelfile = "$InstallPath\Modelfile-gemma3-embed"
@"
FROM $GGUFPath

SYSTEM """Generate semantic embeddings for legal text. Focus on legal concepts, entities, and relationships."""

PARAMETER temperature 0.0
PARAMETER top_p 1.0
PARAMETER num_ctx 2048
"@ | Out-File -FilePath $embedModelfile -Encoding UTF8
& ollama create gemma3-embed -f $embedModelfile

Write-Host "âœ… Local GGUF models configured" -ForegroundColor Green

# ==========================================
# 3. CONFIGURE POSTGRESQL 17 WITH PGAI
# ==========================================

Write-Host "`nðŸ—„ï¸ Configuring PostgreSQL 17 with pgai and RAG schema..." -ForegroundColor Yellow

$env:PGPASSWORD = "postgres123"

# Create comprehensive database schema
$pgaiSchema = @"
-- ==========================================
-- COMPLETE LEGAL AI RAG SYSTEM SCHEMA
-- ==========================================

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS ai CASCADE;
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Configure pgai to use local Ollama
SELECT ai.set_config('ollama.host', 'http://localhost:11434');
SELECT ai.set_config('ollama.timeout', '300');
SELECT ai.set_config('ollama.keep_alive', '5m');

-- Create RAG schema
CREATE SCHEMA IF NOT EXISTS rag;

-- ==========================================
-- DRIZZLE ORM COMPATIBLE TABLES
-- ==========================================

-- Users table (Drizzle compatible)
CREATE TABLE IF NOT EXISTS users (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(255),
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'user',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Cases table
CREATE TABLE IF NOT EXISTS cases (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(500) NOT NULL,
    description TEXT,
    status VARCHAR(50) DEFAULT 'active',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Documents table with RAG enhancements
CREATE TABLE IF NOT EXISTS documents (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    case_id UUID REFERENCES cases(id) ON DELETE CASCADE,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    file_path VARCHAR(1000),
    file_type VARCHAR(50),
    
    -- Full-text search
    content_tsv tsvector GENERATED ALWAYS AS (
        setweight(to_tsvector('english', coalesce(title, '')), 'A') ||
        setweight(to_tsvector('english', coalesce(content, '')), 'B')
    ) STORED,
    
    -- Vector embeddings (768 dimensions for Gemma)
    embedding vector(768),
    
    -- AI-generated fields
    summary JSONB,
    key_points JSONB,
    entities JSONB,
    recommendations JSONB,
    
    -- Processing status
    processing_status VARCHAR(50) DEFAULT 'pending',
    processed_at TIMESTAMPTZ,
    
    -- Metadata
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Document chunks for RAG
CREATE TABLE IF NOT EXISTS document_chunks (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_text TEXT NOT NULL,
    chunk_embedding vector(768),
    chunk_metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(document_id, chunk_index)
);

-- Conversations for context
CREATE TABLE IF NOT EXISTS conversations (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    case_id UUID REFERENCES cases(id) ON DELETE CASCADE,
    title VARCHAR(255),
    context JSONB DEFAULT '[]',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Messages
CREATE TABLE IF NOT EXISTS messages (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    conversation_id UUID REFERENCES conversations(id) ON DELETE CASCADE,
    role VARCHAR(50) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Performance metrics
CREATE TABLE IF NOT EXISTS rag_metrics (
    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
    operation VARCHAR(100) NOT NULL,
    model VARCHAR(100),
    duration_ms INTEGER,
    tokens_used INTEGER,
    success BOOLEAN DEFAULT true,
    error_message TEXT,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ==========================================
-- OPTIMIZED INDEXES
-- ==========================================

CREATE INDEX idx_documents_case_id ON documents(case_id);
CREATE INDEX idx_documents_embedding ON documents USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
CREATE INDEX idx_documents_tsv ON documents USING GIN(content_tsv);
CREATE INDEX idx_documents_status ON documents(processing_status);

CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_embedding ON document_chunks USING ivfflat (chunk_embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX idx_conversations_user_case ON conversations(user_id, case_id);
CREATE INDEX idx_messages_conversation ON messages(conversation_id);

-- ==========================================
-- PGAI FUNCTIONS WITH LOCAL GEMMA3
-- ==========================================

-- Function to generate embeddings using local Gemma3
CREATE OR REPLACE FUNCTION generate_embedding(input_text TEXT)
RETURNS vector
LANGUAGE plpgsql
AS $$
DECLARE
    embedding_result vector;
    start_time TIMESTAMP;
    duration_ms INTEGER;
BEGIN
    start_time := clock_timestamp();
    
    -- Use pgai with local Gemma3 model for embeddings
    SELECT ai.ollama_embed('gemma3-embed', input_text)::vector
    INTO embedding_result;
    
    duration_ms := EXTRACT(MILLISECOND FROM clock_timestamp() - start_time)::INTEGER;
    
    -- Log metrics
    INSERT INTO rag_metrics (operation, model, duration_ms, success)
    VALUES ('generate_embedding', 'gemma3-embed', duration_ms, true);
    
    RETURN embedding_result;
EXCEPTION
    WHEN OTHERS THEN
        INSERT INTO rag_metrics (operation, model, success, error_message)
        VALUES ('generate_embedding', 'gemma3-embed', false, SQLERRM);
        RAISE;
END;
$$;

-- Function to summarize document using local Gemma3
CREATE OR REPLACE FUNCTION summarize_document(content TEXT)
RETURNS JSONB
LANGUAGE plpgsql
AS $$
DECLARE
    summary_result JSONB;
    prompt TEXT;
    start_time TIMESTAMP;
    duration_ms INTEGER;
BEGIN
    start_time := clock_timestamp();
    
    prompt := format('Analyze this legal document and provide a structured summary:

%s

Provide the following in JSON format:
{
  "summary": "2-3 sentence overview",
  "key_points": ["point1", "point2", ...],
  "entities": {
    "persons": ["name1", "name2"],
    "organizations": ["org1", "org2"],
    "dates": ["date1", "date2"],
    "locations": ["loc1", "loc2"]
  },
  "legal_issues": ["issue1", "issue2"],
  "risk_level": "low|medium|high",
  "recommended_actions": ["action1", "action2"]
}', substring(content, 1, 8000));
    
    -- Use local Gemma3 model for summarization
    SELECT ai.ollama_generate(
        'gemma3-summary',
        prompt,
        jsonb_build_object(
            'temperature', 0.1,
            'max_tokens', 1500,
            'format', 'json'
        )
    )::jsonb INTO summary_result;
    
    duration_ms := EXTRACT(MILLISECOND FROM clock_timestamp() - start_time)::INTEGER;
    
    -- Log metrics
    INSERT INTO rag_metrics (operation, model, duration_ms, success)
    VALUES ('summarize_document', 'gemma3-summary', duration_ms, true);
    
    RETURN summary_result;
EXCEPTION
    WHEN OTHERS THEN
        INSERT INTO rag_metrics (operation, model, success, error_message)
        VALUES ('summarize_document', 'gemma3-summary', false, SQLERRM);
        RAISE;
END;
$$;

-- Function to generate recommendations
CREATE OR REPLACE FUNCTION generate_recommendations(
    document_content TEXT,
    document_summary JSONB
)
RETURNS JSONB
LANGUAGE plpgsql
AS $$
DECLARE
    recommendations JSONB;
    prompt TEXT;
BEGIN
    prompt := format('Based on this legal document analysis, provide strategic recommendations:

Summary: %s
Risk Level: %s
Legal Issues: %s

Provide comprehensive recommendations including:
1. Immediate actions required
2. Risk mitigation strategies
3. Legal precedents to consider
4. Evidence to gather
5. Timeline and milestones

Format as JSON with categories: immediate, short_term, long_term, evidence_needed, legal_references',
        document_summary->>'summary',
        document_summary->>'risk_level',
        document_summary->'legal_issues'
    );
    
    SELECT ai.ollama_generate(
        'gemma3-legal',
        prompt,
        jsonb_build_object(
            'temperature', 0.3,
            'max_tokens', 2000
        )
    )::jsonb INTO recommendations;
    
    RETURN recommendations;
END;
$$;

-- Function for chunking documents
CREATE OR REPLACE FUNCTION chunk_document(
    doc_id UUID,
    chunk_size INTEGER DEFAULT 500,
    overlap INTEGER DEFAULT 100
)
RETURNS INTEGER
LANGUAGE plpgsql
AS $$
DECLARE
    doc_content TEXT;
    total_chunks INTEGER := 0;
    chunk_start INTEGER := 1;
    chunk_text TEXT;
    chunk_idx INTEGER := 0;
BEGIN
    -- Get document content
    SELECT content INTO doc_content
    FROM documents WHERE id = doc_id;
    
    IF doc_content IS NULL THEN
        RAISE EXCEPTION 'Document not found: %', doc_id;
    END IF;
    
    -- Clear existing chunks
    DELETE FROM document_chunks WHERE document_id = doc_id;
    
    -- Create chunks with overlap
    WHILE chunk_start <= length(doc_content) LOOP
        chunk_text := substring(doc_content, chunk_start, chunk_size);
        
        -- Insert chunk with embedding
        INSERT INTO document_chunks (
            document_id,
            chunk_index,
            chunk_text,
            chunk_embedding
        ) VALUES (
            doc_id,
            chunk_idx,
            chunk_text,
            generate_embedding(chunk_text)
        );
        
        chunk_idx := chunk_idx + 1;
        total_chunks := total_chunks + 1;
        chunk_start := chunk_start + chunk_size - overlap;
    END LOOP;
    
    RETURN total_chunks;
END;
$$;

-- Enhanced semantic search function
CREATE OR REPLACE FUNCTION semantic_search(
    query_text TEXT,
    case_id UUID DEFAULT NULL,
    limit_results INTEGER DEFAULT 10,
    threshold FLOAT DEFAULT 0.7
)
RETURNS TABLE(
    document_id UUID,
    title VARCHAR(500),
    content TEXT,
    summary JSONB,
    similarity FLOAT,
    relevant_chunks JSONB
)
LANGUAGE plpgsql
AS $$
DECLARE
    query_embedding vector;
BEGIN
    -- Generate embedding for query
    query_embedding := generate_embedding(query_text);
    
    -- Search with hybrid approach (vector + full-text)
    RETURN QUERY
    WITH vector_search AS (
        SELECT 
            d.id,
            d.title,
            d.content,
            d.summary,
            1 - (d.embedding <=> query_embedding) as vector_similarity
        FROM documents d
        WHERE d.embedding IS NOT NULL
            AND (case_id IS NULL OR d.case_id = case_id)
            AND 1 - (d.embedding <=> query_embedding) > threshold
    ),
    text_search AS (
        SELECT 
            d.id,
            ts_rank(d.content_tsv, plainto_tsquery('english', query_text)) as text_rank
        FROM documents d
        WHERE d.content_tsv @@ plainto_tsquery('english', query_text)
            AND (case_id IS NULL OR d.case_id = case_id)
    ),
    chunk_search AS (
        SELECT 
            c.document_id,
            jsonb_agg(
                jsonb_build_object(
                    'chunk_text', substring(c.chunk_text, 1, 200),
                    'similarity', 1 - (c.chunk_embedding <=> query_embedding)
                ) ORDER BY c.chunk_embedding <=> query_embedding
            ) as chunks
        FROM document_chunks c
        WHERE 1 - (c.chunk_embedding <=> query_embedding) > threshold
        GROUP BY c.document_id
    )
    SELECT 
        vs.id,
        vs.title,
        vs.content,
        vs.summary,
        COALESCE(vs.vector_similarity * 0.7 + ts.text_rank * 0.3, vs.vector_similarity) as combined_similarity,
        COALESCE(cs.chunks, '[]'::jsonb)
    FROM vector_search vs
    LEFT JOIN text_search ts ON vs.id = ts.id
    LEFT JOIN chunk_search cs ON vs.id = cs.document_id
    ORDER BY combined_similarity DESC
    LIMIT limit_results;
END;
$$;

-- Function to process complete document
CREATE OR REPLACE FUNCTION process_document_complete(doc_id UUID)
RETURNS JSONB
LANGUAGE plpgsql
AS $$
DECLARE
    doc_record RECORD;
    summary_data JSONB;
    recommendations_data JSONB;
    embedding_vector vector;
    chunks_created INTEGER;
    start_time TIMESTAMP;
    total_duration INTEGER;
BEGIN
    start_time := clock_timestamp();
    
    -- Get document
    SELECT * INTO doc_record FROM documents WHERE id = doc_id;
    
    IF doc_record IS NULL THEN
        RAISE EXCEPTION 'Document not found: %', doc_id;
    END IF;
    
    -- Update status
    UPDATE documents SET processing_status = 'processing' WHERE id = doc_id;
    
    -- Generate summary
    summary_data := summarize_document(doc_record.content);
    
    -- Generate recommendations
    recommendations_data := generate_recommendations(doc_record.content, summary_data);
    
    -- Generate embedding
    embedding_vector := generate_embedding(doc_record.content);
    
    -- Create chunks
    chunks_created := chunk_document(doc_id);
    
    -- Update document with all generated data
    UPDATE documents SET
        summary = summary_data,
        recommendations = recommendations_data,
        embedding = embedding_vector,
        key_points = summary_data->'key_points',
        entities = summary_data->'entities',
        processing_status = 'completed',
        processed_at = NOW(),
        updated_at = NOW()
    WHERE id = doc_id;
    
    total_duration := EXTRACT(MILLISECOND FROM clock_timestamp() - start_time)::INTEGER;
    
    -- Log metrics
    INSERT INTO rag_metrics (operation, model, duration_ms, success, metadata)
    VALUES (
        'process_document_complete',
        'gemma3-legal',
        total_duration,
        true,
        jsonb_build_object(
            'document_id', doc_id,
            'chunks_created', chunks_created,
            'summary_generated', summary_data IS NOT NULL,
            'recommendations_generated', recommendations_data IS NOT NULL
        )
    );
    
    RETURN jsonb_build_object(
        'success', true,
        'document_id', doc_id,
        'summary', summary_data,
        'recommendations', recommendations_data,
        'chunks_created', chunks_created,
        'processing_time_ms', total_duration
    );
END;
$$;

-- Create view for document search
CREATE OR REPLACE VIEW document_search_view AS
SELECT 
    d.id,
    d.title,
    d.case_id,
    c.title as case_title,
    d.summary->>'summary' as summary_text,
    d.summary->>'risk_level' as risk_level,
    d.entities,
    d.processing_status,
    d.created_at,
    d.updated_at
FROM documents d
LEFT JOIN cases c ON d.case_id = c.id;

-- Grant permissions
GRANT ALL ON SCHEMA rag TO postgres;
GRANT ALL ON ALL TABLES IN SCHEMA rag TO postgres;
GRANT ALL ON ALL FUNCTIONS IN SCHEMA rag TO postgres;
"@

# Execute schema creation
$pgaiSchema | & psql -U postgres -h localhost -d legal_ai_db

Write-Host "âœ… Database schema created with pgai functions" -ForegroundColor Green

# ==========================================
# 4. CREATE DRIZZLE ORM SCHEMA
# ==========================================

Write-Host "`nðŸ“ Creating Drizzle ORM schema..." -ForegroundColor Yellow

$drizzleSchema = @"
// drizzle/schema.ts
import { pgTable, uuid, varchar, text, timestamp, jsonb, vector, index, pgSchema } from 'drizzle-orm/pg-core';
import { relations } from 'drizzle-orm';

// Users table
export const users = pgTable('users', {
  id: uuid('id').defaultRandom().primaryKey(),
  email: varchar('email', { length: 255 }).notNull().unique(),
  name: varchar('name', { length: 255 }),
  passwordHash: varchar('password_hash', { length: 255 }).notNull(),
  role: varchar('role', { length: 50 }).default('user'),
  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow()
});

// Cases table
export const cases = pgTable('cases', {
  id: uuid('id').defaultRandom().primaryKey(),
  userId: uuid('user_id').references(() => users.id, { onDelete: 'cascade' }),
  title: varchar('title', { length: 500 }).notNull(),
  description: text('description'),
  status: varchar('status', { length: 50 }).default('active'),
  metadata: jsonb('metadata').default({}),
  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow()
});

// Documents table
export const documents = pgTable('documents', {
  id: uuid('id').defaultRandom().primaryKey(),
  caseId: uuid('case_id').references(() => cases.id, { onDelete: 'cascade' }),
  title: varchar('title', { length: 500 }).notNull(),
  content: text('content').notNull(),
  filePath: varchar('file_path', { length: 1000 }),
  fileType: varchar('file_type', { length: 50 }),
  embedding: vector('embedding', { dimensions: 768 }),
  summary: jsonb('summary'),
  keyPoints: jsonb('key_points'),
  entities: jsonb('entities'),
  recommendations: jsonb('recommendations'),
  processingStatus: varchar('processing_status', { length: 50 }).default('pending'),
  processedAt: timestamp('processed_at'),
  metadata: jsonb('metadata').default({}),
  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow()
}, (table) => ({
  caseIdIdx: index('idx_documents_case_id').on(table.caseId),
  embeddingIdx: index('idx_documents_embedding').on(table.embedding).using('ivfflat'),
  statusIdx: index('idx_documents_status').on(table.processingStatus)
}));

// Document chunks table
export const documentChunks = pgTable('document_chunks', {
  id: uuid('id').defaultRandom().primaryKey(),
  documentId: uuid('document_id').references(() => documents.id, { onDelete: 'cascade' }),
  chunkIndex: integer('chunk_index').notNull(),
  chunkText: text('chunk_text').notNull(),
  chunkEmbedding: vector('chunk_embedding', { dimensions: 768 }),
  chunkMetadata: jsonb('chunk_metadata').default({}),
  createdAt: timestamp('created_at').defaultNow()
}, (table) => ({
  documentIdIdx: index('idx_chunks_document_id').on(table.documentId),
  embeddingIdx: index('idx_chunks_embedding').on(table.chunkEmbedding).using('ivfflat')
}));

// Conversations table
export const conversations = pgTable('conversations', {
  id: uuid('id').defaultRandom().primaryKey(),
  userId: uuid('user_id').references(() => users.id, { onDelete: 'cascade' }),
  caseId: uuid('case_id').references(() => cases.id, { onDelete: 'cascade' }),
  title: varchar('title', { length: 255 }),
  context: jsonb('context').default([]),
  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow()
});

// Messages table
export const messages = pgTable('messages', {
  id: uuid('id').defaultRandom().primaryKey(),
  conversationId: uuid('conversation_id').references(() => conversations.id, { onDelete: 'cascade' }),
  role: varchar('role', { length: 50 }).notNull(),
  content: text('content').notNull(),
  metadata: jsonb('metadata').default({}),
  createdAt: timestamp('created_at').defaultNow()
});

// RAG metrics table
export const ragMetrics = pgTable('rag_metrics', {
  id: uuid('id').defaultRandom().primaryKey(),
  operation: varchar('operation', { length: 100 }).notNull(),
  model: varchar('model', { length: 100 }),
  durationMs: integer('duration_ms'),
  tokensUsed: integer('tokens_used'),
  success: boolean('success').default(true),
  errorMessage: text('error_message'),
  metadata: jsonb('metadata').default({}),
  createdAt: timestamp('created_at').defaultNow()
});

// Relations
export const usersRelations = relations(users, ({ many }) => ({
  cases: many(cases),
  conversations: many(conversations)
}));

export const casesRelations = relations(cases, ({ one, many }) => ({
  user: one(users, {
    fields: [cases.userId],
    references: [users.id]
  }),
  documents: many(documents),
  conversations: many(conversations)
}));

export const documentsRelations = relations(documents, ({ one, many }) => ({
  case: one(cases, {
    fields: [documents.caseId],
    references: [cases.id]
  }),
  chunks: many(documentChunks)
}));

export const documentChunksRelations = relations(documentChunks, ({ one }) => ({
  document: one(documents, {
    fields: [documentChunks.documentId],
    references: [documents.id]
  })
}));

export const conversationsRelations = relations(conversations, ({ one, many }) => ({
  user: one(users, {
    fields: [conversations.userId],
    references: [users.id]
  }),
  case: one(cases, {
    fields: [conversations.caseId],
    references: [cases.id]
  }),
  messages: many(messages)
}));

export const messagesRelations = relations(messages, ({ one }) => ({
  conversation: one(conversations, {
    fields: [messages.conversationId],
    references: [conversations.id]
  })
}));
"@

$drizzleSchema | Out-File -FilePath "$InstallPath\drizzle\schema.ts" -Encoding UTF8

# Create Drizzle config
$drizzleConfig = @"
import type { Config } from 'drizzle-kit';

export default {
  schema: './drizzle/schema.ts',
  out: './drizzle/migrations',
  driver: 'pg',
  dbCredentials: {
    connectionString: process.env.DATABASE_URL || 'postgresql://postgres:postgres123@localhost:5432/legal_ai_db'
  },
  verbose: true,
  strict: true
} satisfies Config;
"@

$drizzleConfig | Out-File -FilePath "$InstallPath\drizzle.config.ts" -Encoding UTF8

Write-Host "âœ… Drizzle ORM schema created" -ForegroundColor Green

# ==========================================
# 5. CREATE ENHANCED RAG SERVICE
# ==========================================

Write-Host "`nðŸš€ Creating Enhanced RAG Service..." -ForegroundColor Yellow

$ragService = @"
// src/services/enhanced-rag-service.ts
import { drizzle } from 'drizzle-orm/postgres-js';
import postgres from 'postgres';
import { documents, documentChunks, cases, conversations, messages, ragMetrics } from '../drizzle/schema';
import { eq, sql, desc, and } from 'drizzle-orm';

export class EnhancedRAGService {
  private db: any;
  private ollamaUrl: string;
  
  constructor() {
    const connectionString = process.env.DATABASE_URL || 'postgresql://postgres:postgres123@localhost:5432/legal_ai_db';
    const client = postgres(connectionString);
    this.db = drizzle(client);
    this.ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
  }

  // Process document with complete pipeline
  async processDocument(documentId: string): Promise<any> {
    const startTime = Date.now();
    
    try {
      // Call PostgreSQL function for complete processing
      const result = await this.db.execute(
        sql\`SELECT process_document_complete(\${documentId}::uuid)\`
      );
      
      const processingResult = result[0].process_document_complete;
      
      // Log success metric
      await this.logMetric('document_processing', 'gemma3-legal', Date.now() - startTime, true);
      
      return processingResult;
    } catch (error) {
      // Log error metric
      await this.logMetric('document_processing', 'gemma3-legal', Date.now() - startTime, false, error.message);
      throw error;
    }
  }

  // Semantic search with hybrid approach
  async semanticSearch(query: string, caseId?: string, limit: number = 10): Promise<any[]> {
    const startTime = Date.now();
    
    try {
      const results = await this.db.execute(
        sql\`SELECT * FROM semantic_search(\${query}, \${caseId}::uuid, \${limit})\`
      );
      
      await this.logMetric('semantic_search', 'gemma3-embed', Date.now() - startTime, true);
      
      return results;
    } catch (error) {
      await this.logMetric('semantic_search', 'gemma3-embed', Date.now() - startTime, false, error.message);
      throw error;
    }
  }

  // Generate recommendations for a case
  async generateCaseRecommendations(caseId: string): Promise<any> {
    const startTime = Date.now();
    
    try {
      // Get all documents for the case
      const caseDocuments = await this.db
        .select()
        .from(documents)
        .where(eq(documents.caseId, caseId));
      
      // Aggregate summaries and generate overall recommendations
      const aggregatedSummary = {
        documents: caseDocuments.map(d => ({
          title: d.title,
          summary: d.summary,
          risk_level: d.summary?.risk_level,
          entities: d.entities
        })),
        total_documents: caseDocuments.length
      };
      
      // Call Ollama for comprehensive recommendations
      const response = await fetch(\`\${this.ollamaUrl}/api/generate\`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'gemma3-legal',
          prompt: \`Based on the following case analysis, provide comprehensive legal recommendations:
          
          \${JSON.stringify(aggregatedSummary, null, 2)}
          
          Provide recommendations for:
          1. Legal strategy
          2. Evidence priorities
          3. Risk mitigation
          4. Timeline and milestones
          5. Resource allocation\`,
          options: {
            temperature: 0.3,
            num_predict: 2000
          }
        })
      });
      
      const recommendations = await response.json();
      
      await this.logMetric('generate_recommendations', 'gemma3-legal', Date.now() - startTime, true);
      
      return recommendations;
    } catch (error) {
      await this.logMetric('generate_recommendations', 'gemma3-legal', Date.now() - startTime, false, error.message);
      throw error;
    }
  }

  // Chat with context
  async chatWithContext(conversationId: string, userMessage: string): Promise<any> {
    const startTime = Date.now();
    
    try {
      // Get conversation context
      const conversation = await this.db
        .select()
        .from(conversations)
        .where(eq(conversations.id, conversationId))
        .limit(1);
      
      const previousMessages = await this.db
        .select()
        .from(messages)
        .where(eq(messages.conversationId, conversationId))
        .orderBy(desc(messages.createdAt))
        .limit(10);
      
      // Search for relevant documents
      const relevantDocs = await this.semanticSearch(userMessage, conversation[0]?.caseId, 5);
      
      // Build context
      const context = {
        conversation_history: previousMessages.reverse(),
        relevant_documents: relevantDocs.map(d => ({
          title: d.title,
          summary: d.summary,
          similarity: d.similarity
        }))
      };
      
      // Generate response
      const response = await fetch(\`\${this.ollamaUrl}/api/generate\`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'gemma3-legal',
          prompt: \`You are a legal AI assistant. Use the following context to answer the user's question:
          
          Context: \${JSON.stringify(context, null, 2)}
          
          User: \${userMessage}
          
          Provide a helpful, accurate response based on the context and your legal knowledge.\`,
          options: {
            temperature: 0.2,
            num_predict: 1500
          }
        })
      });
      
      const aiResponse = await response.json();
      
      // Save messages
      await this.db.insert(messages).values([
        {
          conversationId,
          role: 'user',
          content: userMessage
        },
        {
          conversationId,
          role: 'assistant',
          content: aiResponse.response
        }
      ]);
      
      await this.logMetric('chat_with_context', 'gemma3-legal', Date.now() - startTime, true);
      
      return {
        response: aiResponse.response,
        context: relevantDocs
      };
    } catch (error) {
      await this.logMetric('chat_with_context', 'gemma3-legal', Date.now() - startTime, false, error.message);
      throw error;
    }
  }

  // Batch process documents
  async batchProcessDocuments(documentIds: string[]): Promise<any[]> {
    const results = [];
    
    for (const docId of documentIds) {
      try {
        const result = await this.processDocument(docId);
        results.push({ documentId: docId, ...result });
      } catch (error) {
        results.push({ documentId: docId, error: error.message });
      }
    }
    
    return results;
  }

  // Get processing metrics
  async getMetrics(operation?: string, hours: number = 24): Promise<any[]> {
    const since = new Date(Date.now() - hours * 60 * 60 * 1000);
    
    let query = this.db
      .select()
      .from(ragMetrics)
      .where(sql\`created_at > \${since}\`);
    
    if (operation) {
      query = query.where(eq(ragMetrics.operation, operation));
    }
    
    return await query.orderBy(desc(ragMetrics.createdAt));
  }

  // Log metric
  private async logMetric(
    operation: string,
    model: string,
    durationMs: number,
    success: boolean,
    errorMessage?: string
  ): Promise<void> {
    await this.db.insert(ragMetrics).values({
      operation,
      model,
      durationMs,
      success,
      errorMessage
    });
  }
}

// Export singleton instance
export const ragService = new EnhancedRAGService();
"@

$ragService | Out-File -FilePath "$InstallPath\src\services\enhanced-rag-service.ts" -Encoding UTF8

Write-Host "âœ… Enhanced RAG Service created" -ForegroundColor Green

# ==========================================
# 6. CREATE API ENDPOINTS
# ==========================================

Write-Host "`nðŸŒ Creating API endpoints..." -ForegroundColor Yellow

$apiEndpoints = @"
// src/routes/api/rag/+server.ts
import { json } from '@sveltejs/kit';
import type { RequestHandler } from './$types';
import { ragService } from '../../../services/enhanced-rag-service';

// Process document endpoint
export const POST: RequestHandler = async ({ request, url }) => {
  const action = url.searchParams.get('action');
  
  try {
    const body = await request.json();
    
    switch (action) {
      case 'process':
        const result = await ragService.processDocument(body.documentId);
        return json({ success: true, data: result });
      
      case 'batch-process':
        const batchResult = await ragService.batchProcessDocuments(body.documentIds);
        return json({ success: true, data: batchResult });
      
      case 'search':
        const searchResults = await ragService.semanticSearch(
          body.query,
          body.caseId,
          body.limit || 10
        );
        return json({ success: true, data: searchResults });
      
      case 'chat':
        const chatResponse = await ragService.chatWithContext(
          body.conversationId,
          body.message
        );
        return json({ success: true, data: chatResponse });
      
      case 'recommendations':
        const recommendations = await ragService.generateCaseRecommendations(body.caseId);
        return json({ success: true, data: recommendations });
      
      default:
        return json({ success: false, error: 'Invalid action' }, { status: 400 });
    }
  } catch (error) {
    console.error('RAG API Error:', error);
    return json({ success: false, error: error.message }, { status: 500 });
  }
};

// Get metrics endpoint
export const GET: RequestHandler = async ({ url }) => {
  const operation = url.searchParams.get('operation');
  const hours = parseInt(url.searchParams.get('hours') || '24');
  
  try {
    const metrics = await ragService.getMetrics(operation, hours);
    return json({ success: true, data: metrics });
  } catch (error) {
    return json({ success: false, error: error.message }, { status: 500 });
  }
};
"@

New-Item -ItemType Directory -Force -Path "$InstallPath\src\routes\api\rag" | Out-Null
$apiEndpoints | Out-File -FilePath "$InstallPath\src\routes\api\rag\+server.ts" -Encoding UTF8

Write-Host "âœ… API endpoints created" -ForegroundColor Green

# ==========================================
# 7. CREATE TEST SCRIPT
# ==========================================

Write-Host "`nðŸ§ª Creating test script..." -ForegroundColor Yellow

$testScript = @"
// test-rag-system.mjs
import { config } from 'dotenv';
config();

const API_URL = process.env.API_URL || 'http://localhost:5173/api/rag';

async function testRAGSystem() {
  console.log('ðŸ§ª Testing Enhanced RAG System...\n');
  
  // Test document processing
  console.log('ðŸ“„ Testing document processing...');
  const processResponse = await fetch(\`\${API_URL}?action=process\`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      documentId: 'test-doc-id' // Replace with actual document ID
    })
  });
  
  const processResult = await processResponse.json();
  console.log('Processing result:', processResult);
  
  // Test semantic search
  console.log('\nðŸ” Testing semantic search...');
  const searchResponse = await fetch(\`\${API_URL}?action=search\`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      query: 'contract breach damages',
      limit: 5
    })
  });
  
  const searchResult = await searchResponse.json();
  console.log('Search results:', searchResult.data?.length, 'documents found');
  
  // Test recommendations
  console.log('\nðŸ’¡ Testing recommendations generation...');
  const recsResponse = await fetch(\`\${API_URL}?action=recommendations\`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      caseId: 'test-case-id' // Replace with actual case ID
    })
  });
  
  const recsResult = await recsResponse.json();
  console.log('Recommendations generated:', recsResult.success);
  
  // Test metrics
  console.log('\nðŸ“Š Testing metrics retrieval...');
  const metricsResponse = await fetch(\`\${API_URL}?hours=24\`);
  const metricsResult = await metricsResponse.json();
  console.log('Metrics retrieved:', metricsResult.data?.length, 'records');
  
  console.log('\nâœ… RAG System test complete!');
}

testRAGSystem().catch(console.error);
"@

$testScript | Out-File -FilePath "$InstallPath\test-rag-system.mjs" -Encoding UTF8

Write-Host "âœ… Test script created" -ForegroundColor Green

# ==========================================
# 8. CREATE STARTUP SCRIPT
# ==========================================

Write-Host "`nðŸ“ Creating startup script..." -ForegroundColor Yellow

$startupScript = @"
@echo off
echo ========================================
echo Starting Legal AI RAG System
echo ========================================

:: Start PostgreSQL if not running
net start postgresql-x64-17 2>nul

:: Start Ollama
echo Starting Ollama service...
start /B ollama serve

:: Wait for services
timeout /t 5 /nobreak > nul

:: Start the application
echo Starting application...
cd /d "$InstallPath"
npm run dev

pause
"@

$startupScript | Out-File -FilePath "$InstallPath\start-rag-system.bat" -Encoding ASCII

Write-Host "âœ… Startup script created" -ForegroundColor Green

# ==========================================
# FINAL SUMMARY
# ==========================================

Write-Host "`n" -ForegroundColor Cyan
Write-Host "============================================" -ForegroundColor Cyan
Write-Host "âœ… COMPLETE RAG SYSTEM INSTALLATION SUCCESS!" -ForegroundColor Green
Write-Host "============================================" -ForegroundColor Cyan
Write-Host "`nComponents installed:" -ForegroundColor Yellow
Write-Host "  âœ… pgai extension for PostgreSQL 17" -ForegroundColor Green
Write-Host "  âœ… Local Gemma3 GGUF models (summary, embed, legal)" -ForegroundColor Green
Write-Host "  âœ… Complete database schema with RAG functions" -ForegroundColor Green
Write-Host "  âœ… Drizzle ORM schema and configuration" -ForegroundColor Green
Write-Host "  âœ… Enhanced RAG service with all features" -ForegroundColor Green
Write-Host "  âœ… RESTful API endpoints" -ForegroundColor Green
Write-Host "  âœ… Test scripts" -ForegroundColor Green

Write-Host "`nFeatures enabled:" -ForegroundColor Yellow
Write-Host "  â€¢ Document summarization with local Gemma3" -ForegroundColor Cyan
Write-Host "  â€¢ Semantic embeddings generation" -ForegroundColor Cyan
Write-Host "  â€¢ Hybrid search (vector + full-text)" -ForegroundColor Cyan
Write-Host "  â€¢ Legal recommendations generation" -ForegroundColor Cyan
Write-Host "  â€¢ Contextual chat with RAG" -ForegroundColor Cyan
Write-Host "  â€¢ Document chunking with overlap" -ForegroundColor Cyan
Write-Host "  â€¢ Performance metrics tracking" -ForegroundColor Cyan
Write-Host "  â€¢ Batch document processing" -ForegroundColor Cyan

Write-Host "`nAPI Endpoints:" -ForegroundColor Yellow
Write-Host "  POST /api/rag?action=process       - Process single document" -ForegroundColor Cyan
Write-Host "  POST /api/rag?action=batch-process - Process multiple documents" -ForegroundColor Cyan
Write-Host "  POST /api/rag?action=search        - Semantic search" -ForegroundColor Cyan
Write-Host "  POST /api/rag?action=chat          - Chat with context" -ForegroundColor Cyan
Write-Host "  POST /api/rag?action=recommendations - Generate recommendations" -ForegroundColor Cyan
Write-Host "  GET  /api/rag                       - Get metrics" -ForegroundColor Cyan

Write-Host "`nTo start the system:" -ForegroundColor Yellow
Write-Host "  1. Run: start-rag-system.bat" -ForegroundColor Cyan
Write-Host "  2. Test: node test-rag-system.mjs" -ForegroundColor Cyan

Write-Host "`nLocal GGUF model location:" -ForegroundColor Yellow
Write-Host "  $GGUFPath" -ForegroundColor Cyan

Write-Host "`nðŸŽ‰ Your enhanced RAG system is ready!" -ForegroundColor Green
Write-Host "============================================" -ForegroundColor Cyan
"@