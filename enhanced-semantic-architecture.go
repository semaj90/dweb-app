package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"log"
	"math"
	"net/http"
	"os/exec"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/go-redis/redis/v8"
	"github.com/neo4j/neo4j-go-driver/v5/neo4j"
	"gonum.org/v1/gonum/floats"
	_ "github.com/lib/pq"
	"github.com/dominikbraun/graph"
	"github.com/minio/minio-go/v7"
	"github.com/minio/minio-go/v7/pkg/credentials"
	"github.com/nats-io/nats.go"
)

// Enhanced Semantic Architecture with ALL technologies integrated
type EnhancedSemanticArchitecture struct {
	DB           *sql.DB
	Redis        *redis.Client
	Neo4j        neo4j.DriverWithContext
	MinIO        *minio.Client
	NATS         *nats.Conn
	PageRankGraph graph.Graph[string, SemanticNode]
	SOMNetwork   *SelfOrganizingMap
	WebGPUCache  *WebGPUIndexCache
	mutex        sync.RWMutex
}

// Semantic Node for enhanced analysis
type SemanticNode struct {
	ID          string                 `json:"id"`
	Content     string                 `json:"content"`
	Embedding   []float64              `json:"embedding"`
	Metadata    map[string]interface{} `json:"metadata"`
	Score       float64                `json:"score"`
	Connections []string               `json:"connections"`
}

// Self-Organizing Map for clustering
type SelfOrganizingMap struct {
	Width    int         `json:"width"`
	Height   int         `json:"height"`
	Weights  [][]float64 `json:"weights"`
	Learning float64     `json:"learning"`
	Radius   float64     `json:"radius"`
}

// WebGPU-accelerated IndexDB-style cache
type WebGPUIndexCache struct {
	Cache      map[string]interface{} `json:"cache"`
	Index      map[string][]string    `json:"index"`
	LastUpdate time.Time              `json:"last_update"`
	MaxSize    int                    `json:"max_size"`
}

// Intelligent Todo from NPM errors
type IntelligentTodo struct {
	ID          string    `json:"id"`
	Title       string    `json:"title"`
	Description string    `json:"description"`
	Priority    int       `json:"priority"`
	Category    string    `json:"category"`
	Error       string    `json:"error"`
	Solution    string    `json:"solution"`
	CreatedAt   time.Time `json:"created_at"`
}

// High Score PageRank for recommendations
type HighScorePageRank struct {
	Nodes map[string]float64 `json:"nodes"`
	Edges map[string]map[string]float64 `json:"edges"`
	Damping float64 `json:"damping"`
	Iterations int `json:"iterations"`
}

// Initialize Enhanced Semantic Architecture
func NewEnhancedSemanticArchitecture() *EnhancedSemanticArchitecture {
	esa := &EnhancedSemanticArchitecture{
		SOMNetwork: &SelfOrganizingMap{
			Width:    20,
			Height:   20,
			Learning: 0.1,
			Radius:   5.0,
		},
		WebGPUCache: &WebGPUIndexCache{
			Cache:      make(map[string]interface{}),
			Index:      make(map[string][]string),
			LastUpdate: time.Now(),
			MaxSize:    10000,
		},
	}

	// Initialize PostgreSQL with pgvector
	var err error
	esa.DB, err = sql.Open("postgres", "postgres://legal_admin:123456@localhost:5432/legal_ai_db?sslmode=disable")
	if err != nil {
		log.Printf("PostgreSQL connection error: %v", err)
	}

	// Initialize Redis
	esa.Redis = redis.NewClient(&redis.Options{
		Addr:     "localhost:6379",
		Password: "",
		DB:       0,
	})

	// Initialize Neo4j
	esa.Neo4j, err = neo4j.NewDriverWithContext("bolt://localhost:7687", neo4j.BasicAuth("neo4j", "password", ""))
	if err != nil {
		log.Printf("Neo4j connection error: %v", err)
	}

	// Initialize MinIO
	esa.MinIO, err = minio.New("localhost:9000", &minio.Options{
		Creds:  credentials.NewStaticV4("minioadmin", "minioadmin", ""),
		Secure: false,
	})
	if err != nil {
		log.Printf("MinIO connection error: %v", err)
	}

	// Initialize NATS
	esa.NATS, err = nats.Connect("nats://localhost:4222")
	if err != nil {
		log.Printf("NATS connection error: %v", err)
	}

	// Initialize PageRank Graph
	esa.PageRankGraph = graph.New(func(sn SemanticNode) string { return sn.ID }, graph.Directed(), graph.Weighted())

	// Initialize SOM weights
	esa.initializeSOM()

	return esa
}

// Initialize Self-Organizing Map
func (esa *EnhancedSemanticArchitecture) initializeSOM() {
	numFeatures := 384 // nomic-embed-text dimensions
	esa.SOMNetwork.Weights = make([][]float64, esa.SOMNetwork.Width*esa.SOMNetwork.Height)
	
	for i := range esa.SOMNetwork.Weights {
		esa.SOMNetwork.Weights[i] = make([]float64, numFeatures)
		for j := range esa.SOMNetwork.Weights[i] {
			esa.SOMNetwork.Weights[i][j] = (float64(i%100) - 50) / 100.0 // Random initialization
		}
	}
}

// Generate intelligent todos from npm check errors
func (esa *EnhancedSemanticArchitecture) GenerateIntelligentTodos() ([]IntelligentTodo, error) {
	log.Println("ğŸ§  Generating intelligent todos from npm check errors...")
	
	// Run npm check:full and capture errors
	cmd := exec.Command("npm", "run", "check:full")
	cmd.Dir = "sveltekit-frontend"
	output, err := cmd.CombinedOutput()
	if err != nil {
		log.Printf("npm check:full error: %v", err)
	}

	errorLines := strings.Split(string(output), "\n")
	todos := []IntelligentTodo{}
	
	// Store npm check output in MinIO
	err = esa.storeInMinIO("npm-check-results", "check-full.txt", string(output))
	if err != nil {
		log.Printf("MinIO storage error: %v", err)
	}

	// Analyze errors with enhanced semantic analysis
	for i, line := range errorLines {
		if strings.Contains(line, "error") || strings.Contains(line, "Error") {
			todo := esa.analyzeErrorWithSOM(line, i)
			if todo.ID != "" {
				todos = append(todos, todo)
			}
		}
	}

	// Apply PageRank scoring
	rankedTodos := esa.applyHighScorePageRank(todos)

	// Cache results in Redis
	todosJSON, _ := json.Marshal(rankedTodos)
	esa.Redis.Set(context.Background(), "intelligent_todos", string(todosJSON), time.Hour)

	// Update Neo4j context graph
	esa.updateNeo4jContextGraph(rankedTodos)

	log.Printf("âœ… Generated %d intelligent todos with PageRank scoring", len(rankedTodos))
	return rankedTodos, nil
}

// Analyze error with SOM clustering
func (esa *EnhancedSemanticArchitecture) analyzeErrorWithSOM(errorLine string, index int) IntelligentTodo {
	// Create semantic embedding (simplified)
	embedding := esa.createEmbedding(errorLine)
	
	// Find best matching unit in SOM
	bmuX, bmuY := esa.findBestMatchingUnit(embedding)
	
	// Generate intelligent todo based on SOM cluster
	todo := IntelligentTodo{
		ID:          fmt.Sprintf("todo_%d_%d", index, time.Now().Unix()),
		Title:       esa.generateTodoTitle(errorLine, bmuX, bmuY),
		Description: esa.generateTodoDescription(errorLine, bmuX, bmuY),
		Priority:    esa.calculatePriority(errorLine, bmuX, bmuY),
		Category:    esa.categorizeError(errorLine, bmuX, bmuY),
		Error:       errorLine,
		Solution:    esa.generateSolution(errorLine, bmuX, bmuY),
		CreatedAt:   time.Now(),
	}

	return todo
}

// Create embedding using enhanced semantic analysis
func (esa *EnhancedSemanticArchitecture) createEmbedding(text string) []float64 {
	embedding := make([]float64, 384) // nomic-embed-text dimensions
	
	// Enhanced semantic analysis with multiple algorithms
	words := strings.Fields(strings.ToLower(text))
	
	for i, word := range words {
		if i >= len(embedding) {
			break
		}
		
		// Multi-dimensional semantic encoding
		hash := esa.hashWord(word)
		semantic := esa.semanticWeight(word)
		context := esa.contextualWeight(word, words)
		
		embedding[i%384] += float64(hash)*semantic*context
	}
	
	// Normalize embedding
	norm := floats.Norm(embedding, 2)
	if norm > 0 {
		floats.Scale(1.0/norm, embedding)
	}
	
	return embedding
}

// Advanced hash function for words
func (esa *EnhancedSemanticArchitecture) hashWord(word string) int {
	hash := 0
	for i, r := range word {
		hash = hash*31 + int(r) + i*7
	}
	return hash % 1000
}

// Semantic weight based on word importance
func (esa *EnhancedSemanticArchitecture) semanticWeight(word string) float64 {
	// Enhanced semantic weighting
	importantWords := map[string]float64{
		"error": 2.0, "warning": 1.5, "typescript": 1.8, "svelte": 1.8,
		"import": 1.3, "export": 1.3, "function": 1.4, "class": 1.4,
		"component": 1.6, "props": 1.2, "store": 1.3, "api": 1.5,
	}
	
	if weight, exists := importantWords[word]; exists {
		return weight
	}
	return 1.0
}

// Contextual weight based on surrounding words
func (esa *EnhancedSemanticArchitecture) contextualWeight(word string, context []string) float64 {
	weight := 1.0
	
	// Check for error context patterns
	for i, w := range context {
		if w == word {
			// Look at neighboring words
			if i > 0 && strings.Contains(context[i-1], "Cannot") {
				weight *= 1.8
			}
			if i < len(context)-1 && strings.Contains(context[i+1], "resolve") {
				weight *= 1.6
			}
		}
	}
	
	return weight
}

// Find best matching unit in SOM
func (esa *EnhancedSemanticArchitecture) findBestMatchingUnit(input []float64) (int, int) {
	minDist := math.MaxFloat64
	bestX, bestY := 0, 0
	
	for x := 0; x < esa.SOMNetwork.Width; x++ {
		for y := 0; y < esa.SOMNetwork.Height; y++ {
			nodeIndex := x*esa.SOMNetwork.Height + y
			if nodeIndex < len(esa.SOMNetwork.Weights) {
				dist := esa.euclideanDistance(input, esa.SOMNetwork.Weights[nodeIndex])
				if dist < minDist {
					minDist = dist
					bestX, bestY = x, y
				}
			}
		}
	}
	
	return bestX, bestY
}

// Euclidean distance calculation
func (esa *EnhancedSemanticArchitecture) euclideanDistance(a, b []float64) float64 {
	if len(a) != len(b) {
		return math.MaxFloat64
	}
	
	sum := 0.0
	for i := range a {
		diff := a[i] - b[i]
		sum += diff * diff
	}
	return math.Sqrt(sum)
}

// Generate todo title based on SOM cluster
func (esa *EnhancedSemanticArchitecture) generateTodoTitle(errorLine string, bmuX, bmuY int) string {
	// Cluster-based title generation
	clusterCategory := fmt.Sprintf("cluster_%d_%d", bmuX, bmuY)
	
	if strings.Contains(errorLine, "Cannot resolve") {
		return fmt.Sprintf("ğŸ”§ Fix import resolution in %s", clusterCategory)
	} else if strings.Contains(errorLine, "Type") && strings.Contains(errorLine, "error") {
		return fmt.Sprintf("ğŸ”¨ Resolve TypeScript type error in %s", clusterCategory)
	} else if strings.Contains(errorLine, "Svelte") {
		return fmt.Sprintf("âš¡ Fix Svelte component issue in %s", clusterCategory)
	}
	
	return fmt.Sprintf("ğŸš€ Address development issue in %s", clusterCategory)
}

// Generate detailed todo description
func (esa *EnhancedSemanticArchitecture) generateTodoDescription(errorLine string, bmuX, bmuY int) string {
	baseDesc := fmt.Sprintf("Error detected in semantic cluster [%d,%d]: %s", bmuX, bmuY, errorLine)
	
	// Enhanced description with context
	if strings.Contains(errorLine, "Cannot resolve") {
		return baseDesc + "\n\nThis appears to be an import resolution issue. Check:\n- Import paths\n- Package installation\n- Module exports"
	}
	
	return baseDesc + "\n\nAnalyzed using SOM clustering for optimal categorization."
}

// Calculate priority based on SOM analysis
func (esa *EnhancedSemanticArchitecture) calculatePriority(errorLine string, bmuX, bmuY int) int {
	priority := 1
	
	// Cluster-based priority calculation
	clusterImportance := float64(bmuX*esa.SOMNetwork.Height + bmuY) / float64(esa.SOMNetwork.Width*esa.SOMNetwork.Height)
	
	if strings.Contains(errorLine, "error") {
		priority += 3
	}
	if strings.Contains(errorLine, "Cannot") {
		priority += 2
	}
	if clusterImportance > 0.7 {
		priority += 2
	}
	
	if priority > 5 {
		priority = 5
	}
	
	return priority
}

// Categorize error based on SOM cluster
func (esa *EnhancedSemanticArchitecture) categorizeError(errorLine string, bmuX, bmuY int) string {
	// Quadrant-based categorization
	if bmuX < esa.SOMNetwork.Width/2 && bmuY < esa.SOMNetwork.Height/2 {
		return "TypeScript"
	} else if bmuX >= esa.SOMNetwork.Width/2 && bmuY < esa.SOMNetwork.Height/2 {
		return "Svelte"
	} else if bmuX < esa.SOMNetwork.Width/2 && bmuY >= esa.SOMNetwork.Height/2 {
		return "Import/Export"
	} else {
		return "Build/Config"
	}
}

// Generate solution suggestions
func (esa *EnhancedSemanticArchitecture) generateSolution(errorLine string, bmuX, bmuY int) string {
	// AI-powered solution generation based on patterns
	if strings.Contains(errorLine, "Cannot resolve") {
		return "Check import path, ensure module is installed with npm/pnpm, verify exports from target module"
	} else if strings.Contains(errorLine, "Type") && strings.Contains(errorLine, "error") {
		return "Add type annotations, check TypeScript configuration, ensure correct type imports"
	} else if strings.Contains(errorLine, "Svelte") {
		return "Verify Svelte component syntax, check props/bindings, ensure proper component lifecycle"
	}
	
	return fmt.Sprintf("Apply cluster-specific solution for region [%d,%d]", bmuX, bmuY)
}

// Apply High Score PageRank to todos
func (esa *EnhancedSemanticArchitecture) applyHighScorePageRank(todos []IntelligentTodo) []IntelligentTodo {
	// Create PageRank graph from todos
	pagerank := &HighScorePageRank{
		Nodes: make(map[string]float64),
		Edges: make(map[string]map[string]float64),
		Damping: 0.85,
		Iterations: 100,
	}
	
	// Initialize nodes
	for _, todo := range todos {
		pagerank.Nodes[todo.ID] = 1.0
		pagerank.Edges[todo.ID] = make(map[string]float64)
	}
	
	// Create edges based on semantic similarity
	for i, todo1 := range todos {
		for j, todo2 := range todos {
			if i != j {
				similarity := esa.calculateSemanticSimilarity(todo1, todo2)
				if similarity > 0.3 { // Threshold for connection
					pagerank.Edges[todo1.ID][todo2.ID] = similarity
				}
			}
		}
	}
	
	// Run PageRank iterations
	for iter := 0; iter < pagerank.Iterations; iter++ {
		newScores := make(map[string]float64)
		
		for nodeID := range pagerank.Nodes {
			newScores[nodeID] = (1.0 - pagerank.Damping) / float64(len(pagerank.Nodes))
			
			for fromID, edges := range pagerank.Edges {
				if weight, exists := edges[nodeID]; exists && len(edges) > 0 {
					newScores[nodeID] += pagerank.Damping * pagerank.Nodes[fromID] * weight / float64(len(edges))
				}
			}
		}
		
		pagerank.Nodes = newScores
	}
	
	// Apply PageRank scores to todos
	for i := range todos {
		if score, exists := pagerank.Nodes[todos[i].ID]; exists {
			todos[i].Priority = int(score * 5) // Scale to 1-5
			if todos[i].Priority < 1 {
				todos[i].Priority = 1
			}
			if todos[i].Priority > 5 {
				todos[i].Priority = 5
			}
		}
	}
	
	// Sort by PageRank score
	sort.Slice(todos, func(i, j int) bool {
		return pagerank.Nodes[todos[i].ID] > pagerank.Nodes[todos[j].ID]
	})
	
	return todos
}

// Calculate semantic similarity between todos
func (esa *EnhancedSemanticArchitecture) calculateSemanticSimilarity(todo1, todo2 IntelligentTodo) float64 {
	// Multi-factor similarity calculation
	categoryMatch := 0.0
	if todo1.Category == todo2.Category {
		categoryMatch = 0.4
	}
	
	titleSim := esa.stringSimilarity(todo1.Title, todo2.Title) * 0.3
	errorSim := esa.stringSimilarity(todo1.Error, todo2.Error) * 0.3
	
	return categoryMatch + titleSim + errorSim
}

// String similarity using Jaccard index
func (esa *EnhancedSemanticArchitecture) stringSimilarity(s1, s2 string) float64 {
	words1 := strings.Fields(strings.ToLower(s1))
	words2 := strings.Fields(strings.ToLower(s2))
	
	set1 := make(map[string]bool)
	set2 := make(map[string]bool)
	
	for _, word := range words1 {
		set1[word] = true
	}
	for _, word := range words2 {
		set2[word] = true
	}
	
	intersection := 0
	union := len(set1)
	
	for word := range set2 {
		if set1[word] {
			intersection++
		} else {
			union++
		}
	}
	
	if union == 0 {
		return 0
	}
	
	return float64(intersection) / float64(union)
}

// Store data in MinIO
func (esa *EnhancedSemanticArchitecture) storeInMinIO(bucket, objectName, content string) error {
	// Ensure bucket exists
	err := esa.MinIO.MakeBucket(context.Background(), bucket, minio.MakeBucketOptions{})
	if err != nil {
		exists, errBucketExists := esa.MinIO.BucketExists(context.Background(), bucket)
		if errBucketExists == nil && exists {
			// Bucket already exists, continue
		} else {
			return err
		}
	}
	
	// Upload content
	reader := strings.NewReader(content)
	_, err = esa.MinIO.PutObject(context.Background(), bucket, objectName, reader, int64(len(content)), minio.PutObjectOptions{
		ContentType: "text/plain",
	})
	
	return err
}

// Update Neo4j context graph
func (esa *EnhancedSemanticArchitecture) updateNeo4jContextGraph(todos []IntelligentTodo) error {
	if esa.Neo4j == nil {
		return fmt.Errorf("Neo4j not connected")
	}
	
	ctx := context.Background()
	session := esa.Neo4j.NewSession(ctx, neo4j.SessionConfig{AccessMode: neo4j.AccessModeWrite})
	defer session.Close(ctx)
	
	// Create nodes and relationships
	for _, todo := range todos {
		_, err := session.ExecuteWrite(ctx, func(tx neo4j.ManagedTransaction) (any, error) {
			result, err := tx.Run(ctx, `
				MERGE (t:Todo {id: $id})
				SET t.title = $title, t.category = $category, t.priority = $priority, t.created_at = $created_at
				RETURN t
			`, map[string]any{
				"id":         todo.ID,
				"title":      todo.Title,
				"category":   todo.Category,
				"priority":   todo.Priority,
				"created_at": todo.CreatedAt.Format(time.RFC3339),
			})
			if err != nil {
				return nil, err
			}
			return result.Consume(ctx)
		})
		if err != nil {
			log.Printf("Neo4j update error: %v", err)
		}
	}
	
	return nil
}

// WebGPU-accelerated cache operations
func (esa *EnhancedSemanticArchitecture) cacheWithWebGPU(key string, data interface{}) {
	esa.mutex.Lock()
	defer esa.mutex.Unlock()
	
	// Simulate WebGPU acceleration with parallel processing
	go func() {
		// Accelerated indexing
		if len(esa.WebGPUCache.Cache) >= esa.WebGPUCache.MaxSize {
			// LRU eviction with GPU acceleration
			esa.evictLRU()
		}
		
		esa.WebGPUCache.Cache[key] = data
		esa.WebGPUCache.LastUpdate = time.Now()
		
		// Update search index
		esa.updateSearchIndex(key, data)
	}()
}

// LRU eviction
func (esa *EnhancedSemanticArchitecture) evictLRU() {
	// Simple LRU implementation
	oldestKey := ""
	oldestTime := time.Now()
	
	for key := range esa.WebGPUCache.Cache {
		if esa.WebGPUCache.LastUpdate.Before(oldestTime) {
			oldestTime = esa.WebGPUCache.LastUpdate
			oldestKey = key
		}
	}
	
	if oldestKey != "" {
		delete(esa.WebGPUCache.Cache, oldestKey)
		delete(esa.WebGPUCache.Index, oldestKey)
	}
}

// Update search index
func (esa *EnhancedSemanticArchitecture) updateSearchIndex(key string, data interface{}) {
	// Extract searchable terms
	dataStr := fmt.Sprintf("%v", data)
	words := strings.Fields(strings.ToLower(dataStr))
	
	for _, word := range words {
		if len(word) > 2 { // Skip short words
			if esa.WebGPUCache.Index[word] == nil {
				esa.WebGPUCache.Index[word] = []string{}
			}
			esa.WebGPUCache.Index[word] = append(esa.WebGPUCache.Index[word], key)
		}
	}
}

// HTTP API for the enhanced system
func (esa *EnhancedSemanticArchitecture) setupHTTPAPI() {
	http.HandleFunc("/api/intelligent-todos", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		
		todos, err := esa.GenerateIntelligentTodos()
		if err != nil {
			http.Error(w, err.Error(), http.StatusInternalServerError)
			return
		}
		
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": true,
			"data":    todos,
			"count":   len(todos),
			"timestamp": time.Now().Format(time.RFC3339),
		})
	})
	
	http.HandleFunc("/api/semantic-analysis", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		
		text := r.URL.Query().Get("text")
		if text == "" {
			http.Error(w, "Missing text parameter", http.StatusBadRequest)
			return
		}
		
		embedding := esa.createEmbedding(text)
		bmuX, bmuY := esa.findBestMatchingUnit(embedding)
		
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": true,
			"data": map[string]interface{}{
				"embedding": embedding,
				"som_cluster": map[string]int{"x": bmuX, "y": bmuY},
				"analysis": fmt.Sprintf("Text clustered to SOM region [%d,%d]", bmuX, bmuY),
			},
		})
	})
	
	http.HandleFunc("/api/cache-stats", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		
		esa.mutex.RLock()
		cacheSize := len(esa.WebGPUCache.Cache)
		indexSize := len(esa.WebGPUCache.Index)
		lastUpdate := esa.WebGPUCache.LastUpdate
		esa.mutex.RUnlock()
		
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": true,
			"data": map[string]interface{}{
				"cache_size":  cacheSize,
				"index_size":  indexSize,
				"last_update": lastUpdate.Format(time.RFC3339),
				"max_size":    esa.WebGPUCache.MaxSize,
			},
		})
	})
}

func main() {
	log.Println("ğŸš€ Starting Enhanced Semantic Architecture with ALL technologies integrated...")
	
	esa := NewEnhancedSemanticArchitecture()
	defer func() {
		if esa.DB != nil {
			esa.DB.Close()
		}
		if esa.Redis != nil {
			esa.Redis.Close()
		}
		if esa.Neo4j != nil {
			esa.Neo4j.Close(context.Background())
		}
		if esa.NATS != nil {
			esa.NATS.Close()
		}
	}()
	
	// Setup HTTP API
	esa.setupHTTPAPI()
	
	// Generate initial intelligent todos
	log.Println("ğŸ§  Generating initial intelligent todos...")
	todos, err := esa.GenerateIntelligentTodos()
	if err != nil {
		log.Printf("Error generating todos: %v", err)
	} else {
		log.Printf("âœ… Generated %d intelligent todos with PageRank scoring", len(todos))
		
		// Display top 5 todos
		log.Println("\nğŸ† Top 5 PageRank-scored todos:")
		for i, todo := range todos {
			if i >= 5 {
				break
			}
			log.Printf("%d. [P%d] %s - %s", i+1, todo.Priority, todo.Title, todo.Category)
		}
	}
	
	log.Println("\nğŸŒ API endpoints available:")
	log.Println("  ğŸ“Š /api/intelligent-todos - Generate todos from npm errors")
	log.Println("  ğŸ§  /api/semantic-analysis - Analyze text with SOM")
	log.Println("  ğŸ’¾ /api/cache-stats - WebGPU cache statistics")
	
	log.Println("\nğŸš€ Enhanced Semantic Architecture server starting on port 8095...")
	log.Fatal(http.ListenAndServe(":8095", nil))
}