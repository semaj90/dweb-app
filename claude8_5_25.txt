BullMQ Job Queue │ │ (Redis-backed) │ └────────────────────┬────────────────────────────────────┘ │ ┌────────────────────▼────────────────────────────────────┐ │ Go GPU Server (Port 8080) │ │ • Document Processing • Entity Extraction │ │ • Risk Assessment • Vector Embeddings
make go redis on windows server backed? make sure ollama uses gpu, cublas and cuda. make it avaliable to the entire app, bits-ui, melt-ui builders, xstate if detect state change. get back to our neo4j setup for recommendations from when user prosecutor updates, crud their cases, uploaded evidence, generates reports, if aiAssistant.svelte api is used? melt-ui triggers. json encoded to get, post, and service_worker, service_thread.
now uptae with redis, bullmq, and use goroutine with our setup here. attempt to read the #codebase to figure it out. ai assistant use local llm ollama to ai summarize elements using api requests, loki.js caching, fuse.js, make sure all typescript barrel stores updated with redis, bullmq, ssr hydration and subscribed properly. utilize @context7 generate_best_practices go, node.js, postgresql, pg vector, qdrant, bits-ui, unocss, melt-ui. javascript.  we have have enhanced rag setup.
make sure sxstate works for sveletekit frontend api json requests, since go gpu server make sure build json and a router? server? script to server json back and parse simd? until nomic-embed, and pgvector and qdrant all update their respective datastores. typescript barrell stores attached to our sveltekit architecture. make sure sveltekit vite knows this with go, node.js cluster and the rest, we'll be doing vertex buffer here. all windows native. make a todo comment if get stuck, with a *timestampsummaryphase1314.md