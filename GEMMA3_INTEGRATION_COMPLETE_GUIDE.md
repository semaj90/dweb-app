# Gemma3 GGUF Model Integration Guide
## Complete Setup & Documentation for Legal AI Assistant

This guide provides comprehensive documentation for integrating a custom Unsloth-trained Gemma3 GGUF model into your SvelteKit 5 Legal AI system with Bits UI components.

## ğŸ¯ Overview

Your system architecture:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SvelteKit 5   â”‚â”€â”€â”€â”€â–¶â”‚   Ollama API    â”‚â”€â”€â”€â”€â–¶â”‚  Gemma3 GGUF    â”‚
â”‚   + Bits UI     â”‚     â”‚  (Port 11434)   â”‚     â”‚  mo16.gguf      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alternative:   â”‚     â”‚   vLLM Server   â”‚
â”‚  Direct vLLM    â”‚â”€â”€â”€â”€â–¶â”‚  (Port 8001)    â”‚
â”‚  Integration    â”‚     â”‚  API Compatible â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Current File Structure

```
c:/Users/james/Desktop/deeds-web/deeds-web-app/
â”œâ”€â”€ gemma3Q4_K_M/
â”‚   â””â”€â”€ mo16.gguf                           # Your trained model
â”œâ”€â”€ sveltekit-frontend/
â”‚   â”œâ”€â”€ src/lib/
â”‚   â”‚   â”œâ”€â”€ components/ai/
â”‚   â”‚   â”‚   â”œâ”€â”€ AIChatInterface.svelte      # Main chat UI
â”‚   â”‚   â”‚   â”œâ”€â”€ AIButton.svelte             # Chat trigger
â”‚   â”‚   â”‚   â””â”€â”€ AIChatMessage.svelte        # Message display
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama-service.ts           # Ollama integration
â”‚   â”‚   â”‚   â””â”€â”€ tauri-llm.ts               # Local LLM service
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â”œâ”€â”€ gemma3-config.ts           # Model configurations
â”‚   â”‚       â””â”€â”€ local-llm.ts               # LLM paths & settings
â”‚   â””â”€â”€ src/routes/api/
â”‚       â”œâ”€â”€ ai/
â”‚       â”‚   â”œâ”€â”€ chat/+server.ts            # Chat API endpoint
â”‚       â”‚   â”œâ”€â”€ ask/+server.ts             # RAG endpoint
â”‚       â”‚   â””â”€â”€ ollama-gemma3/+server.ts   # Direct Ollama
â”‚       â””â”€â”€ ...
â”œâ”€â”€ setup-gemma3-legal.ps1                # Ollama setup script
â”œâ”€â”€ Modelfile-Ultra-Light                 # Ollama model config
â”œâ”€â”€ direct-gemma3-vllm-server.py          # vLLM server
â”œâ”€â”€ test-gemma3-direct.py                 # Model testing
â””â”€â”€ ...
```

## ğŸš€ Quick Start

### Method 1: Ollama Integration (Recommended)

1. **Ensure Ollama is running:**
```powershell
# Start Ollama service
ollama serve
```

2. **Run the setup script:**
```powershell
cd "c:/Users/james/Desktop/deeds-web/deeds-web-app"
./setup-gemma3-legal.ps1
```

3. **Verify installation:**
```powershell
ollama list
ollama run gemma3-legal "What are the key elements of contract law?"
```

### Method 2: Direct vLLM Integration

1. **Install dependencies:**
```bash
pip install vllm fastapi uvicorn
# or fallback: pip install llama-cpp-python
```

2. **Start the vLLM server:**
```bash
python direct-gemma3-vllm-server.py
```

3. **Test the server:**
```bash
python test-gemma3-direct.py
```

## ğŸ”§ GGUF Model Conversion

If you need to convert your model to GGUF format:

```python
# convert.py - Unsloth to GGUF conversion
from unsloth import FastLanguageModel
import torch

# Load your trained model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="path/to/your/unsloth-model",
    max_seq_length=8192,
    dtype=None,
    load_in_4bit=True,
)

# Export to GGUF with Q4_K_M quantization
model.save_pretrained_gguf(
    "gemma3-legal", 
    tokenizer,
    quantization_method="q4_k_m",
    # For legal AI, use higher precision
    use_fast_tokenizer=True,
)
```

### Quantization Options:
- **Q4_K_M**: Best balance (recommended) - ~4GB RAM
- **Q8_0**: Higher precision - ~6GB RAM  
- **Q4_0**: Faster inference - ~3GB RAM

## ğŸ—ï¸ SvelteKit Frontend Integration

### 1. Chat Interface Component

Your `AIChatInterface.svelte` is well-structured. Key features:

```typescript
// Current configuration in the component
let settings: ChatSettings = {
  model: 'gemma3-legal:latest',  // âœ… Points to custom model
  temperature: 0.1,              // âœ… Good for legal precision
  maxTokens: 512,                // âœ… Reasonable for responses
  topP: 0.9,                     // âœ… Balanced creativity
  systemPrompt: 'You are a specialized Legal AI...' // âœ… Legal context
};
```

### 2. API Endpoints Status

Current endpoints need completion:

```typescript
// âœ… EXISTS: src/routes/api/ai/ollama-gemma3/+server.ts
// âš ï¸  MOCK: src/routes/api/ai/chat/+server.ts (needs real integration)
// âœ… PARTIAL: src/routes/api/ai/ask/+server.ts (has Ollama integration)
```

### 3. Service Layer Integration

Your `ollama-service.ts` includes:

```typescript
class OllamaService {
  // âœ… Model detection with fallbacks
  private async detectGemma3Model(): Promise<void> {
    const customLegalModel = this.availableModels.find(
      (model) => model.name === "gemma3-legal"
    );
    
    if (customLegalModel) {
      this.gemma3Model = "gemma3-legal";
      console.log(`ğŸ›ï¸ Using custom legal AI model: ${this.gemma3Model}`);
      return;
    }
    // ... fallback logic
  }

  // âœ… GGUF import functionality
  async importGGUF(modelPath: string, modelName = "gemma3-legal"): Promise<boolean>
  
  // âœ… Generation with legal-optimized settings
  async generate(prompt: string, options = {}): Promise<string>
}
```

## ğŸ¨ Bits UI Integration

Your components use modern Bits UI patterns:

```svelte
<!-- AIChatInterface.svelte - Using Bits UI patterns -->
<script lang="ts">
  import { createDialog, melt } from '@melt-ui/svelte';
  import { fade, fly, scale } from 'svelte/transition';
  
  // âœ… Proper Bits UI dialog setup
  const {
    elements: { trigger, overlay, content, title, close },
    states: { open }
  } = createDialog();
</script>

<!-- âœ… Accessible, draggable chat window -->
<div class="yorha-chat-window" bind:this={windowElement}>
  <!-- Chat content with legal AI branding -->
</div>
```

## ğŸ”— Complete Integration Steps

### Step 1: Fix API Endpoints

Update `src/routes/api/ai/chat/+server.ts`:

```typescript
import { json } from "@sveltejs/kit";
import { ollamaService } from "$lib/services/ollama-service";

export const POST = async ({ request }) => {
  try {
    const { message, context, model = "gemma3-legal" } = await request.json();
    
    if (!message) {
      return json({ error: "Message is required" }, { status: 400 });
    }

    // Use actual Ollama service instead of mock
    const response = await ollamaService.generate(message, {
      system: "You are a specialized Legal AI Assistant...",
      temperature: 0.1,
      maxTokens: 512,
    });
    
    return json({ 
      response,
      model,
      metadata: {
        provider: "ollama",
        confidence: 0.9,
        executionTime: Date.now() - startTime,
      }
    });
  } catch (error) {
    console.error("AI chat error:", error);
    return json({ error: "Failed to process chat" }, { status: 500 });
  }
};
```

### Step 2: Update Frontend Chat Component

Ensure your `AIChatInterface.svelte` calls the right endpoint:

```typescript
async function sendMessage(content: string) {
  try {
    const response = await fetch('/api/ai/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        message: content,
        model: settings.model, // "gemma3-legal"
        temperature: settings.temperature,
        maxTokens: settings.maxTokens,
        systemPrompt: settings.systemPrompt
      })
    });

    const data = await response.json();
    if (data.error) throw new Error(data.error);
    
    addMessage('assistant', data.response);
  } catch (error) {
    addMessage('assistant', 'Sorry, I encountered an error processing your request.', true);
  }
}
```

### Step 3: Enhanced Model Configuration

Your `gemma3-config.ts` provides excellent legal-specific configurations:

```typescript
export const LEGAL_SYSTEM_PROMPTS = {
  general: `You are a specialized legal AI assistant with expertise in:
- Case law analysis and legal research
- Document review and evidence evaluation  
- Legal reasoning and argumentation
- Citation and precedent identification
- Procedural guidance and compliance`,

  case_analysis: `You are analyzing legal case documents. Focus on:
- Key facts and legal issues
- Applicable laws and regulations
- Relevant precedents and citations
- Potential arguments and counterarguments
- Risk assessment and recommendations`,

  // ... more specialized prompts
};
```

## ğŸ§ª Testing & Troubleshooting

### Test Scripts Available:

1. **Basic Integration Test:**
```bash
node sveltekit-frontend/scripts/test-ai-integration.js
```

2. **Direct Model Test:**
```bash
python test-gemma3-direct.py
```

3. **Frontend Test Pages:**
- `/test-gemma3` - Model status and testing
- `/test-ai-ask` - RAG system testing  
- `/ai-test` - Full integration testing

### Common Issues & Solutions:

#### 1. Model Not Loading
```powershell
# Check if model file exists
Test-Path ".\gemma3Q4_K_M\mo16.gguf"

# Verify Ollama service
ollama list
curl http://localhost:11434/api/tags
```

#### 2. GGUF Import Fails
```powershell
# Update Modelfile path (currently using /tmp/mo16.gguf)
# Should be: .\gemma3Q4_K_M\mo16.gguf
```

#### 3. Frontend Not Connecting
```typescript
// Check API endpoint URLs in components
apiEndpoint = 'http://localhost:11434/api/generate'  // Ollama
// or
apiEndpoint = 'http://localhost:8001/v1/chat/completions'  // vLLM
```

#### 4. Memory Issues
- Minimum 4GB RAM for Q4_K_M quantization
- Close other applications if needed
- Consider Q4_0 quantization for lower memory usage

## ğŸ“Š Performance Optimization

### Model Settings for Legal AI:

```typescript
// Precise legal analysis
const preciseSettings = {
  temperature: 0.1,    // Low creativity, high consistency
  topP: 0.7,          // Focused responses
  topK: 10,           // Limited token selection
  repeatPenalty: 1.05 // Prevent repetition
};

// Balanced legal consultation  
const balancedSettings = {
  temperature: 0.7,    // More natural responses
  topP: 0.9,          // Broader context
  topK: 40,           // More varied vocabulary
  repeatPenalty: 1.1  // Standard repetition control
};
```

### Context Window Management:

```typescript
// Your current config supports:
contextLength: 8192,     // Good for document analysis
maxTokens: 4096,        // Reasonable response length
```

## ğŸ¨ Advanced UI Features

Your system includes sophisticated UI components:

### 1. Draggable Chat Window
```svelte
<!-- Already implemented in AIChatInterface.svelte -->
<div class="yorha-chat-window" class:dragging={isDragging}>
  <!-- YoRHa-inspired design with legal AI branding -->
</div>
```

### 2. Model Selection Dropdown
```svelte
<select bind:value={settings.model}>
  <option value="gemma3-legal:latest">Gemma3 Legal (Q4_K_M)</option>
  <option value="llama3.2:1b">Llama3.2 Fallback</option>
</select>
```

### 3. Streaming Responses
```typescript
// Your AIChatInterface supports streaming
let isTyping = false;
// Implement actual streaming via Server-Sent Events or WebSocket
```

## ğŸš¦ Status Dashboard

Current implementation status:

### âœ… Completed
- [x] GGUF model file ready (`mo16.gguf`)
- [x] Ollama service integration (`ollama-service.ts`)
- [x] Frontend chat components (`AIChatInterface.svelte`)
- [x] Legal-specific configurations (`gemma3-config.ts`)
- [x] Setup scripts (`setup-gemma3-legal.ps1`)
- [x] Testing utilities
- [x] Bits UI integration
- [x] YoRHa design theme

### âš ï¸ Needs Completion
- [ ] API endpoint mock removal (`/api/ai/chat/+server.ts`)
- [ ] Streaming response implementation
- [ ] Model health monitoring
- [ ] Error handling improvements
- [ ] RAG integration completion

### ğŸ”§ Modelfile Fixes Needed
```plaintext
# Current Modelfile-Ultra-Light has wrong path:
FROM /tmp/mo16.gguf  # âŒ Wrong path

# Should be:
FROM .\gemma3Q4_K_M\mo16.gguf  # âœ… Correct path
```

## ğŸ¯ Next Steps

1. **Fix Modelfile path** and re-run setup script
2. **Update chat API** to use real Ollama integration  
3. **Test model responses** with legal queries
4. **Implement streaming** for better UX
5. **Add model health monitoring** to UI

## ğŸ“š Documentation Links

- **Local Setup Guide:** `markdowns/LOCAL_LLM_SETUP.md`
- **Component Documentation:** Bits UI components in `src/lib/components/ui/`
- **API Documentation:** Endpoints in `src/routes/api/`
- **Configuration Files:** `src/lib/config/`

Your system has excellent foundation architecture. The main missing piece is connecting the mock API endpoints to the actual Ollama service, which is already well-implemented in your service layer.
