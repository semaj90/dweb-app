{
  "timestamp": "2025-07-29 22:27:32",
  "system": {
    "python_version": "3.13.5",
    "platform": "nt",
    "torch_available": true,
    "cuda_available": true
  },
  "vllm": {
    "native_installation": false,
    "mock_server_running": false,
    "openai_compatible": false
  },
  "docker": {
    "docker_running": true,
    "ollama_available": true
  },
  "test_results": {
    "system_requirements": true,
    "api_endpoints": false,
    "completion_test": false
  },
  "recommendations": [
    "vLLM native installation failed - using mock server"
  ]
}