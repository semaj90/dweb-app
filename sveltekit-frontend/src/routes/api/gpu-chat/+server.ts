import os from "os";
import net from "net";
/**
 * GPU-Optimized Production System with CUDA/TensorRT
 * Integrates Go microservices, Ollama, LangChain, NVIDIA toolkit
 * Port 5173 with automatic fallback
 */

import { type RequestHandler,  json } from '@sveltejs/kit';
import { WebSocketServer } from "ws";
import { promises as fs } from "fs";
import { // Dynamic port selection const PRIMARY_PORT = 5173; const WS_PORT_OFFSET = 2; // WebSocket on 5175 let currentPort = PRIMARY_PORT; let wsPort = PRIMARY_PORT + WS_PORT_OFFSET; // Service endpoints const SERVICES = { ollama: 'http://localhost:11434', goMicroservice: 'http://localhost:8084', gpuOrchestrator: 'http://localhost:8085', tensorRT: 'http://localhost:8086', langchain: 'http://localhost:8087', neo4j: 'bolt://localhost:7687', redis: 'redis://localhost:6379', minio: 'http://localhost:9000', nats: 'nats://localhost:4222', grpc: 'localhost:50051' // gRPC with TLS }; // GPU/CUDA configuration const GPU_CONFIG = { cuda: { version: '12.0', deviceId: 0, memory: 12 * 1024 * 1024 * 1024, // RTX 3060 12GB cores: 3584, tensorCores: 112 }, tensorRT: { enabled: true, precision: 'FP16', // FP16 for RTX 3060 workspace: 2 * 1024 * 1024 * 1024, // 2GB workspace batchSize: 32 }, cudnn: { version: '8.9', enabled: true } }; // WebSocket server instance let wss: WebSocketServer | null = null; const clients = new Map<string, any>(); // Initialize WebSocket async function initializeWebSocket() { if (wss) return; try { // Try primary WebSocket port const portAvailable = await checkPort(wsPort); if (!portAvailable) { wsPort = await findAvailablePort(wsPort); } wss = new WebSocketServer({ port: wsPort }); wss.on('connection', (ws, req) => { const clientId = generateClientId(); const ip = req.socket.remoteAddress; console.log(`Client connected: ${clientId} from ${ip}`); clients.set(clientId, { ws, id: clientId, ip, connectedAt: new Date() }); ws.on('message', async (data) => { try { const message = JSON.parse(data.toString()); await handleWebSocketMessage(clientId, message); } catch (error) { console.error('WebSocket error:', error); ws.send(JSON.stringify({ type: 'error', error: 'Invalid message format' })); } }); ws.on('close', () => { clients.delete(clientId); console.log(`Client disconnected: ${clientId}`); }); // Send connection acknowledgment ws.send(JSON.stringify({ type: 'connected', clientId, wsPort, services: SERVICES, gpuConfig: GPU_CONFIG })); }); console.log(`WebSocket server running on port ${wsPort}`); } catch (error) { console.error('WebSocket initialization failed:', error); } } // Generate unique client ID function generateClientId(): string { return `client_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`; } // Handle WebSocket messages async function handleWebSocketMessage(clientId: string, message: any) { const client = clients.get(clientId); if (!client) return; switch (message.type) { case 'chat': const response = await processWithGPU(message.content); client.ws.send(JSON.stringify({ type: 'chat_response', ...response })); break; case 'batch': const results = await processBatch(message.items); client.ws.send(JSON.stringify({ type: 'batch_complete', results })); break; case 'status': const status = await getSystemStatus(); client.ws.send(JSON.stringify({ type: 'status_response', ...status })); break; } } // Process with GPU acceleration async function processWithGPU(content: string): Promise<any> { try { // Try Go microservice with GPU const response = await fetch(`${SERVICES.goMicroservice}/api/process`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ content, gpu: true, tensorRT: GPU_CONFIG.tensorRT.enabled, batchSize: GPU_CONFIG.tensorRT.batchSize }) }); if (response.ok) { return await response.json(); } } catch (error) { console.error('Go microservice error:', error); } // Fallback to Ollama try { const ollamaResponse = await fetch(`${SERVICES.ollama}/api/generate`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ model: 'gemma3:legal-latest', prompt: content, options: { num_gpu: 35, // GPU layers for RTX 3060 temperature: 0.7, num_predict: 500 } }) }); return await ollamaResponse.json(); } catch (error) { console.error('Ollama error:', error); return { response: 'Service temporarily unavailable', error: true }; } } // Batch processing with CUDA async function processBatch(items: any[]): Promise<any[]> { try { const response = await fetch(`${SERVICES.goMicroservice}/api/batch`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ items, parallel: true, gpuBatch: GPU_CONFIG.tensorRT.batchSize }) }); return await response.json(); } catch (error) { console.error('Batch processing error:', error); return items.map(item => ({ ...item, error: 'Processing failed' })); } } // Get system status async function getSystemStatus(): Promise<any> { const status: any = { timestamp: new Date().toISOString(), port: currentPort, wsPort, services: {}, gpu: null, system: { platform: os.platform(), arch: os.arch(), cpus: os.cpus().length, memory: { total: os.totalmem(), free: os.freemem() } } }; // Check services for (const [name, url] of Object.entries(SERVICES)) { if (name === 'grpc') continue; try { const checkUrl = url.startsWith('http') ? url : `http://${url.split('://')[1]}`; const response = await fetch(`${checkUrl}/health`, { signal: AbortSignal.timeout(1000) }); status.services[name] = response.ok; } catch { status.services[name] = false; } } // Check GPU try { const gpuResponse = await fetch(`${SERVICES.gpuOrchestrator}/api/gpu-status`); if (gpuResponse.ok) { status.gpu = await gpuResponse.json(); } } catch { status.gpu = GPU_CONFIG; } return status; } // Check if port is available async function checkPort(port: number): Promise<boolean> { return new Promise((resolve) => { const net = require('net'); const server = net.createServer(); server.once('error', () => resolve(false)); server.once('listening', () => { server.close(); resolve(true); }); server.listen(port); }); } // Find available port async function findAvailablePort(startPort: number): Promise<number> { for (let port = startPort; port < startPort + 10; port++) { if (await checkPort(port)) { return port; } } return startPort + Math.floor(Math.random() * 1000); } // Initialize services on module load initializeWebSocket().catch(console.error); // Main chat endpoint export const POST: RequestHandler = async ({ request }) => { const { message, sessionId, options } = await request.json(); try { const response = await processWithGPU(message); return json({ success: true, response: response.response || response.content || '', metadata: { model: response.model || 'gemma3:legal-latest', processingTime: response.processingTime, gpuUsed: true, tensorRT: GPU_CONFIG.tensorRT.enabled, port: currentPort, wsPort } }); } catch (error) { console.error('Chat error:', error); return json({ success: false, error: 'Processing failed', fallback: true }, { status: 500 }); } }; // Health check endpoint export const GET: RequestHandler = async () => { const status = await getSystemStatus(); return json({ healthy: true, ...status }); };