import { concurrentSearch } from './concurrent-indexeddb-search';

interface FlashAttentionConfig {
  gpu_device: number;
  memory_limit: number;
  attention_heads: number;
  sequence_length: number;
  batch_size: number;
  precision: 'fp16' | 'fp32';
  optimization_level: 'O1' | 'O2' | 'O3';
}

interface GPUErrorBatch {
  id: string;
  errors: TypeScriptError[];
  priority: 'low' | 'medium' | 'high' | 'critical';
  processing_strategy: 'parallel' | 'sequential' | 'hybrid';
  model: 'gemma3-legal:latest';
  expected_tokens: number;
}

interface TypeScriptError {
  code: string;
  message: string;
  file: string;
  line: number;
  column: number;
  severity: 'error' | 'warning' | 'info';
  category: 'syntax' | 'type' | 'import' | 'binding' | 'svelte5' | 'unknown';
}

interface GPUProcessingResult {
  batchId: string;
  fixes: ErrorFix[];
  performance: {
    processing_time_ms: number;
    gpu_utilization: number;
    memory_usage_mb: number;
    tokens_per_second: number;
  };
  status: 'completed' | 'partial' | 'failed';
}

interface ErrorFix {
  errorId: string;
  originalCode: string;
  fixedCode: string;
  confidence: number;
  explanation: string;
  category: string;
}

export class FlashAttentionGPUErrorProcessor {
  private config: FlashAttentionConfig;
  private isInitialized = false;
  private processingQueue: GPUErrorBatch[] = [];
  private activeProcessing = false;

  constructor() {
    this.config = {
      gpu_device: 0,
      memory_limit: 8192,
      attention_heads: 32,
      sequence_length: 4096,
      batch_size: 8,
      precision: 'fp16',
      optimization_level: 'O2'
    };
  }

  async initialize(): Promise<void> {
    if (this.isInitialized) return;

    try {
      await this.initializeGPU();
      await this.validateModels();
      await concurrentSearch.initialize();
      
      this.isInitialized = true;
      console.log('‚ö° FlashAttention2 GPU Error Processor initialized');
      console.log(`üéØ GPU Device: ${this.config.gpu_device}, Memory: ${this.config.memory_limit}MB`);
    } catch (error) {
      console.error('‚ùå Failed to initialize GPU processor:', error);
      throw error;
    }
  }

  private async initializeGPU(): Promise<void> {
    const gpuResponse = await fetch('http://localhost:5173/api/gpu/devices');
    if (!gpuResponse.ok) {
      throw new Error('GPU not available');
    }

    const gpuInfo = await gpuResponse.json();
    console.log('üöÄ GPU Info:', gpuInfo);
  }

  private async validateModels(): Promise<void> {
    const modelsResponse = await fetch('http://localhost:11434/api/tags');
    if (!modelsResponse.ok) {
      throw new Error('Ollama not available');
    }

    const models = await modelsResponse.json();
    const requiredModels = ['gemma3-legal:latest', 'nomic-embed-text:latest'];
    
    for (const requiredModel of requiredModels) {
      const found = models.models?.some((m: any) => m.name === requiredModel);
      if (!found) {
        throw new Error(`Required model ${requiredModel} not found`);
      }
    }

    console.log('‚úÖ Required models validated');
  }

  async processErrors(errors: TypeScriptError[]): Promise<GPUProcessingResult> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    const batchId = `batch-${Date.now()}`;
    const startTime = performance.now();

    console.log(`üî• Processing ${errors.length} errors with FlashAttention2...`);

    try {
      const categorizedErrors = this.categorizeErrors(errors);
      const batch = this.createErrorBatch(batchId, categorizedErrors);
      
      const fixes = await this.processErrorBatch(batch);
      
      const endTime = performance.now();
      const processingTime = endTime - startTime;

      const result: GPUProcessingResult = {
        batchId,
        fixes,
        performance: {
          processing_time_ms: processingTime,
          gpu_utilization: await this.getGPUUtilization(),
          memory_usage_mb: await this.getMemoryUsage(),
          tokens_per_second: this.calculateTokensPerSecond(fixes.length, processingTime)
        },
        status: 'completed'
      };

      console.log(`‚ö° Batch ${batchId} completed in ${processingTime.toFixed(2)}ms`);
      console.log(`üéØ Generated ${fixes.length} fixes with ${result.performance.tokens_per_second.toFixed(1)} tokens/sec`);

      return result;
    } catch (error) {
      console.error(`‚ùå Error processing batch ${batchId}:`, error);
      return {
        batchId,
        fixes: [],
        performance: {
          processing_time_ms: performance.now() - startTime,
          gpu_utilization: 0,
          memory_usage_mb: 0,
          tokens_per_second: 0
        },
        status: 'failed'
      };
    }
  }

  private categorizeErrors(errors: TypeScriptError[]): TypeScriptError[] {
    return errors.map(error => ({
      ...error,
      category: this.detectErrorCategory(error.code, error.message)
    }));
  }

  private detectErrorCategory(code: string, message: string): TypeScriptError['category'] {
    if (message.includes('export let') || message.includes('$props')) return 'svelte5';
    if (code.startsWith('TS2307') || message.includes('Cannot find module')) return 'import';
    if (code.startsWith('TS2322') || message.includes('Type')) return 'type';
    if (message.includes('syntax') || message.includes('Unexpected')) return 'syntax';
    if (message.includes('bind:') || message.includes('on:')) return 'binding';
    return 'unknown';
  }

  private createErrorBatch(batchId: string, errors: TypeScriptError[]): GPUErrorBatch {
    const priority = errors.length > 1000 ? 'critical' : 
                    errors.length > 500 ? 'high' :
                    errors.length > 100 ? 'medium' : 'low';

    return {
      id: batchId,
      errors,
      priority,
      processing_strategy: 'hybrid',
      model: 'gemma3-legal:latest',
      expected_tokens: errors.length * 150
    };
  }

  private async processErrorBatch(batch: GPUErrorBatch): Promise<ErrorFix[]> {
    const fixes: ErrorFix[] = [];
    const batchSize = this.config.batch_size;
    
    for (let i = 0; i < batch.errors.length; i += batchSize) {
      const errorChunk = batch.errors.slice(i, i + batchSize);
      const chunkFixes = await this.processErrorChunk(errorChunk, batch.id);
      fixes.push(...chunkFixes);
      
      console.log(`‚ö° Processed chunk ${Math.floor(i / batchSize) + 1}/${Math.ceil(batch.errors.length / batchSize)}`);
    }

    return fixes;
  }

  private async processErrorChunk(errors: TypeScriptError[], batchId: string): Promise<ErrorFix[]> {
    const fixes: ErrorFix[] = [];

    for (const error of errors) {
      try {
        const fix = await this.generateErrorFix(error, batchId);
        if (fix) {
          fixes.push(fix);
        }
      } catch (fixError) {
        console.error(`‚ùå Failed to fix error ${error.code}:`, fixError);
      }
    }

    return fixes;
  }

  private async generateErrorFix(error: TypeScriptError, batchId: string): Promise<ErrorFix | null> {
    const contextResults = await concurrentSearch.search({
      query: `${error.code} ${error.message} ${error.category}`,
      filters: { language: ['typescript', 'svelte'] },
      options: { threshold: 0.4, maxResults: 5 }
    });

    const contextText = contextResults
      .map(result => `File: ${result.path}\n${result.content}`)
      .join('\n\n');

    const prompt = this.buildFixPrompt(error, contextText);
    
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'gemma3-legal:latest',
        prompt,
        stream: false,
        options: {
          temperature: 0.1,
          top_p: 0.9,
          max_tokens: 500
        }
      })
    });

    if (!response.ok) {
      console.error(`‚ùå Ollama API error: ${response.status}`);
      return null;
    }

    const result = await response.json();
    return this.parseFixResponse(error, result.response);
  }

  private buildFixPrompt(error: TypeScriptError, context: string): string {
    const categoryPrompts = {
      svelte5: `Fix this Svelte 5 migration error. Use $props() instead of export let, $state() for reactivity, $derived() for computed values.`,
      import: `Fix this import error. Check the file path and module existence.`,
      type: `Fix this TypeScript type error. Ensure proper type annotations and compatibility.`,
      syntax: `Fix this syntax error. Check for missing semicolons, brackets, or quotes.`,
      binding: `Fix this Svelte binding error. Use proper event binding syntax.`,
      unknown: `Analyze and fix this error based on the context provided.`
    };

    return `You are an expert TypeScript and Svelte developer. Fix this error:

Error: ${error.code} - ${error.message}
File: ${error.file}:${error.line}:${error.column}
Category: ${error.category}

${categoryPrompts[error.category]}

Context from similar files:
${context}

Provide ONLY the corrected code snippet that fixes this specific error. Do not include explanations or markdown formatting.`;
  }

  private parseFixResponse(error: TypeScriptError, response: string): ErrorFix {
    const lines = response.split('\n');
    const codeStartIndex = lines.findIndex(line => line.includes('```') || line.trim().startsWith('//'));
    
    let fixedCode = response;
    if (codeStartIndex !== -1) {
      fixedCode = lines.slice(codeStartIndex)
        .join('\n')
        .replace(/```[a-z]*\n?/g, '')
        .trim();
    }

    return {
      errorId: `${error.file}:${error.line}:${error.column}`,
      originalCode: `// Line ${error.line}: ${error.message}`,
      fixedCode,
      confidence: this.calculateConfidence(error.category, response.length),
      explanation: `Fixed ${error.category} error: ${error.code}`,
      category: error.category
    };
  }

  private calculateConfidence(category: string, responseLength: number): number {
    const baseConfidence = {
      svelte5: 0.9,
      import: 0.8,
      type: 0.7,
      syntax: 0.9,
      binding: 0.8,
      unknown: 0.5
    };

    let confidence = baseConfidence[category as keyof typeof baseConfidence] || 0.5;
    
    if (responseLength < 50) confidence *= 0.7;
    if (responseLength > 200) confidence *= 0.9;
    
    return Math.min(confidence, 0.95);
  }

  private async getGPUUtilization(): Promise<number> {
    try {
      const response = await fetch('http://localhost:5173/api/gpu/metrics');
      if (response.ok) {
        const metrics = await response.json();
        return metrics.utilization || 0;
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Could not get GPU utilization:', error);
    }
    return 0;
  }

  private async getMemoryUsage(): Promise<number> {
    try {
      const response = await fetch('http://localhost:5173/api/gpu/memory-status');
      if (response.ok) {
        const status = await response.json();
        return status.used_mb || 0;
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Could not get memory usage:', error);
    }
    return 0;
  }

  private calculateTokensPerSecond(fixCount: number, processingTimeMs: number): number {
    const avgTokensPerFix = 150;
    const totalTokens = fixCount * avgTokensPerFix;
    return (totalTokens / processingTimeMs) * 1000;
  }

  async processLiveErrors(): Promise<GPUProcessingResult> {
    console.log('üîç Scanning for live TypeScript errors...');
    
    const checkResponse = await fetch('http://localhost:5173/api/check');
    if (!checkResponse.ok) {
      throw new Error('Failed to get TypeScript check results');
    }

    const checkResults = await checkResponse.text();
    const errors = this.parseTypeScriptOutput(checkResults);
    
    console.log(`üìä Found ${errors.length} TypeScript errors`);
    
    await concurrentSearch.indexTypeScriptErrors(errors);
    
    const result = await this.processErrors(errors);
    
    console.log(`üéØ FlashAttention2 processing complete:`);
    console.log(`   - Fixes generated: ${result.fixes.length}`);
    console.log(`   - GPU utilization: ${result.performance.gpu_utilization}%`);
    console.log(`   - Processing speed: ${result.performance.tokens_per_second.toFixed(1)} tokens/sec`);
    
    return result;
  }

  private parseTypeScriptOutput(output: string): TypeScriptError[] {
    const errorLines = output.split('\n').filter(line => 
      line.includes('TS') && (line.includes('error') || line.includes('warning'))
    );

    return errorLines.map((line, index) => {
      const tsCodeMatch = line.match(/TS(\d+)/);
      const fileMatch = line.match(/([^(]+)\((\d+),(\d+)\)/);
      
      return {
        code: tsCodeMatch ? `TS${tsCodeMatch[1]}` : `TS-${index}`,
        message: line.split(': ').slice(1).join(': ').trim(),
        file: fileMatch ? fileMatch[1].trim() : 'unknown',
        line: fileMatch ? parseInt(fileMatch[2]) : 0,
        column: fileMatch ? parseInt(fileMatch[3]) : 0,
        severity: line.includes('error') ? 'error' as const : 'warning' as const,
        category: this.detectErrorCategory(tsCodeMatch ? `TS${tsCodeMatch[1]}` : '', line)
      };
    });
  }

  async optimizeBatchProcessing(errors: TypeScriptError[]): Promise<GPUErrorBatch[]> {
    const batches: GPUErrorBatch[] = [];
    const errorsByCategory = this.groupErrorsByCategory(errors);
    
    for (const [category, categoryErrors] of Object.entries(errorsByCategory)) {
      const priority = this.calculatePriority(category, categoryErrors.length);
      const batchSize = this.calculateOptimalBatchSize(categoryErrors.length);
      
      for (let i = 0; i < categoryErrors.length; i += batchSize) {
        const batchErrors = categoryErrors.slice(i, i + batchSize);
        const batch: GPUErrorBatch = {
          id: `${category}-batch-${i / batchSize}-${Date.now()}`,
          errors: batchErrors,
          priority,
          processing_strategy: this.selectProcessingStrategy(batchErrors.length),
          model: 'gemma3-legal:latest',
          expected_tokens: batchErrors.length * 150
        };
        
        batches.push(batch);
      }
    }

    return batches.sort((a, b) => this.getPriorityWeight(b.priority) - this.getPriorityWeight(a.priority));
  }

  private groupErrorsByCategory(errors: TypeScriptError[]): Record<string, TypeScriptError[]> {
    const groups: Record<string, TypeScriptError[]> = {};
    
    for (const error of errors) {
      if (!groups[error.category]) {
        groups[error.category] = [];
      }
      groups[error.category].push(error);
    }
    
    return groups;
  }

  private calculatePriority(category: string, count: number): GPUErrorBatch['priority'] {
    if (category === 'syntax' || count > 500) return 'critical';
    if (category === 'svelte5' || count > 200) return 'high';
    if (category === 'type' || count > 50) return 'medium';
    return 'low';
  }

  private calculateOptimalBatchSize(errorCount: number): number {
    if (errorCount > 1000) return 16;
    if (errorCount > 500) return 12;
    if (errorCount > 100) return 8;
    return 4;
  }

  private selectProcessingStrategy(batchSize: number): GPUErrorBatch['processing_strategy'] {
    if (batchSize > 12) return 'parallel';
    if (batchSize > 6) return 'hybrid';
    return 'sequential';
  }

  private getPriorityWeight(priority: GPUErrorBatch['priority']): number {
    const weights = { critical: 4, high: 3, medium: 2, low: 1 };
    return weights[priority];
  }

  async getFlashAttentionStatus(): Promise<{
    gpu_available: boolean;
    model_loaded: boolean;
    memory_usage: number;
    processing_queue_size: number;
    last_processing_time: number;
  }> {
    const [gpuStatus, memoryStatus] = await Promise.all([
      this.checkGPUStatus(),
      this.getMemoryUsage()
    ]);

    return {
      gpu_available: gpuStatus,
      model_loaded: this.isInitialized,
      memory_usage: memoryStatus,
      processing_queue_size: this.processingQueue.length,
      last_processing_time: 0
    };
  }

  private async checkGPUStatus(): Promise<boolean> {
    try {
      const response = await fetch('http://localhost:5173/api/gpu/cuda-status');
      if (response.ok) {
        const status = await response.json();
        return status.cuda_available && status.devices.length > 0;
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è GPU status check failed:', error);
    }
    return false;
  }

  async runFlashAttentionBenchmark(): Promise<{
    processing_speed: number;
    memory_efficiency: number;
    accuracy_score: number;
    gpu_utilization: number;
  }> {
    console.log('üß™ Running FlashAttention2 benchmark...');
    
    const testErrors: TypeScriptError[] = [
      {
        code: 'TS2322',
        message: "Type 'string' is not assignable to type 'number'",
        file: 'test.ts',
        line: 1,
        column: 5,
        severity: 'error',
        category: 'type'
      },
      {
        code: 'TS2307',
        message: "Cannot find module 'nonexistent'",
        file: 'test.ts',
        line: 2,
        column: 1,
        severity: 'error',
        category: 'import'
      }
    ];

    const startTime = performance.now();
    const result = await this.processErrors(testErrors);
    const endTime = performance.now();

    const benchmarkResults = {
      processing_speed: result.performance.tokens_per_second,
      memory_efficiency: 1 - (result.performance.memory_usage_mb / this.config.memory_limit),
      accuracy_score: result.fixes.reduce((acc, fix) => acc + fix.confidence, 0) / result.fixes.length,
      gpu_utilization: result.performance.gpu_utilization
    };

    console.log('üìä FlashAttention2 Benchmark Results:');
    console.log(`   - Processing Speed: ${benchmarkResults.processing_speed.toFixed(1)} tokens/sec`);
    console.log(`   - Memory Efficiency: ${(benchmarkResults.memory_efficiency * 100).toFixed(1)}%`);
    console.log(`   - Accuracy Score: ${(benchmarkResults.accuracy_score * 100).toFixed(1)}%`);
    console.log(`   - GPU Utilization: ${benchmarkResults.gpu_utilization.toFixed(1)}%`);

    return benchmarkResults;
  }

  destroy(): void {
    this.processingQueue = [];
    this.activeProcessing = false;
    this.isInitialized = false;
    console.log('üõë FlashAttention2 GPU Error Processor destroyed');
  }
}

export const flashAttentionProcessor = new FlashAttentionGPUErrorProcessor();