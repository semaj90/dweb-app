/**
 * Enhanced Caching Optimizer - Legal AI Performance Suite
 * 
 * Advanced caching optimization with:
 * - Warm cache management with predictive loading
 * - Dynamic TTL tuning based on access patterns
 * - Request batching with GPU utilization optimization
 * - Cache hit rate monitoring and analytics
 * - Legal document priority-based caching
 */

import { EventEmitter } from 'events';
import { createClient, type RedisClientType } from 'redis';

interface CacheWarmerConfig {
  warmupSchedule: {
    commonQueries: string[];
    documentTypes: string[];
    userPatterns: string[];
  };
  priorities: {
    legal: number;
    evidence: number;
    reports: number;
    searches: number;
  };
  performance: {
    batchSize: number;
    maxConcurrency: number;
    gpuUtilizationTarget: number; // 0.0 to 1.0
  };
}

interface CacheMetrics {
  hitRate: number;
  missRate: number;
  evictionRate: number;
  averageLatency: number;
  gpuUtilization: number;
  memoryPressure: number;
  topQueries: Array<{query: string; count: number; avgLatency: number}>;
  lastOptimized: Date;
}

interface TTLStrategy {
  documentType: string;
  accessFrequency: number;
  lastAccessed: Date;
  computedTTL: number;
  priority: 'critical' | 'high' | 'medium' | 'low';
}

export class EnhancedCachingOptimizer extends EventEmitter {
  private redis: RedisClientType;
  private metrics: CacheMetrics;
  private ttlStrategies: Map<string, TTLStrategy>;
  private requestBatcher: RequestBatcher;
  private warmupTimer: NodeJS.Timer | null;
  private config: CacheWarmerConfig;

  constructor(config: Partial<CacheWarmerConfig> = {}) {
    super();
    
    this.config = {
      warmupSchedule: {
        commonQueries: [
          'legal precedent search',
          'evidence correlation',
          'case timeline analysis',
          'document similarity',
          'legal citation lookup'
        ],
        documentTypes: ['evidence', 'legal_brief', 'case_file', 'report', 'citation'],
        userPatterns: ['recent_documents', 'frequent_searches', 'active_cases'],
        ...config.warmupSchedule
      },
      priorities: {
        legal: 0.9,      // Highest priority
        evidence: 0.8,   // High priority  
        reports: 0.6,    // Medium priority
        searches: 0.7,   // Medium-high priority
        ...config.priorities
      },
      performance: {
        batchSize: 50,
        maxConcurrency: 10,
        gpuUtilizationTarget: 0.85, // Target 85% GPU utilization
        ...config.performance
      }
    };

    this.metrics = this.initializeMetrics();
    this.ttlStrategies = new Map();
    this.requestBatcher = new RequestBatcher(this.config.performance);
    this.warmupTimer = null;

    this.initializeRedis();
    this.startCacheOptimization();
  }

  private initializeMetrics(): CacheMetrics {
    return {
      hitRate: 0,
      missRate: 0,
      evictionRate: 0,
      averageLatency: 0,
      gpuUtilization: 0,
      memoryPressure: 0,
      topQueries: [],
      lastOptimized: new Date()
    };
  }

  private async initializeRedis() {
    try {
      this.redis = createClient({
        url: 'redis://localhost:6379',
        socket: {
          reconnectStrategy: (retries) => Math.min(retries * 50, 500)
        }
      });

      this.redis.on('error', (err) => {
        console.error('‚ùå Redis Cache Optimizer Error:', err);
        this.emit('redis_error', err);
      });

      this.redis.on('connect', () => {
        console.log('‚úÖ Cache Optimizer connected to Redis');
        this.emit('redis_connected');
      });

      await this.redis.connect();
      
      // Subscribe to cache events for real-time optimization
      await this.setupCacheEventListeners();
      
    } catch (error) {
      console.error('‚ùå Failed to initialize Redis for cache optimization:', error);
      this.emit('initialization_error', error);
    }
  }

  private async setupCacheEventListeners() {
    const subscriber = this.redis.duplicate();
    await subscriber.connect();
    
    // Listen for cache events
    await subscriber.subscribe('cache:hit', (message) => {
      this.handleCacheHit(JSON.parse(message));
    });
    
    await subscriber.subscribe('cache:miss', (message) => {
      this.handleCacheMiss(JSON.parse(message));
    });

    await subscriber.subscribe('gpu:utilization', (message) => {
      this.handleGPUUtilization(JSON.parse(message));
    });
  }

  /**
   * 1. WARM CACHE WITH COMMON QUERIES
   */
  async warmCache(): Promise<void> {
    console.log('üî• Starting intelligent cache warming...');
    
    const warmupTasks = [];
    
    // Warm common queries
    for (const query of this.config.warmupSchedule.commonQueries) {
      warmupTasks.push(this.preloadQuery(query));
    }
    
    // Warm document types by priority
    for (const docType of this.config.warmupSchedule.documentTypes) {
      const priority = this.config.priorities[docType as keyof typeof this.config.priorities] || 0.5;
      warmupTasks.push(this.preloadDocumentType(docType, priority));
    }
    
    // Warm user patterns
    for (const pattern of this.config.warmupSchedule.userPatterns) {
      warmupTasks.push(this.preloadUserPattern(pattern));
    }
    
    // Execute warmup tasks in batches
    const results = await this.requestBatcher.executeBatch(warmupTasks);
    
    console.log(`‚úÖ Cache warming completed: ${results.successful}/${results.total} tasks successful`);
    this.emit('cache_warmed', results);
  }

  private async preloadQuery(query: string): Promise<void> {
    const cacheKey = `query:${this.hashQuery(query)}`;
    const exists = await this.redis.exists(cacheKey);
    
    if (!exists) {
      // Simulate query execution and cache result
      const result = await this.executeQueryForCache(query);
      const ttl = this.calculateOptimalTTL('search', query);
      
      await this.redis.setEx(cacheKey, ttl, JSON.stringify(result));
      console.log(`üîç Pre-cached query: ${query} (TTL: ${ttl}s)`);
    }
  }

  private async preloadDocumentType(docType: string, priority: number): Promise<void> {
    // Get most accessed documents of this type
    const recentDocs = await this.getRecentDocumentsByType(docType, Math.ceil(50 * priority));
    
    for (const doc of recentDocs) {
      const cacheKey = `doc:${doc.id}`;
      const exists = await this.redis.exists(cacheKey);
      
      if (!exists) {
        const ttl = this.calculateOptimalTTL(docType, doc.id);
        await this.redis.setEx(cacheKey, ttl, JSON.stringify(doc));
      }
    }
    
    console.log(`üìÑ Pre-cached ${recentDocs.length} documents of type: ${docType}`);
  }

  /**
   * 2. DYNAMIC TTL TUNING BASED ON ACCESS PATTERNS
   */
  calculateOptimalTTL(type: string, key: string): number {
    const strategy = this.ttlStrategies.get(key);
    const baseTTL = this.getBaseTTL(type);
    
    if (!strategy) {
      // First time access - use base TTL
      this.ttlStrategies.set(key, {
        documentType: type,
        accessFrequency: 1,
        lastAccessed: new Date(),
        computedTTL: baseTTL,
        priority: this.inferPriority(type)
      });
      return baseTTL;
    }
    
    // Calculate frequency-adjusted TTL
    const hoursSinceLastAccess = (Date.now() - strategy.lastAccessed.getTime()) / (1000 * 60 * 60);
    const frequencyMultiplier = Math.min(strategy.accessFrequency / 10, 3); // Max 3x multiplier
    const recencyMultiplier = Math.max(1 - (hoursSinceLastAccess / 24), 0.1); // Decay over 24 hours
    
    let computedTTL = baseTTL * frequencyMultiplier * recencyMultiplier;
    
    // Apply priority boost
    const priorityMultiplier = this.getPriorityMultiplier(strategy.priority);
    computedTTL *= priorityMultiplier;
    
    // Clamp between reasonable bounds
    computedTTL = Math.max(300, Math.min(computedTTL, 86400)); // 5 minutes to 24 hours
    
    // Update strategy
    strategy.computedTTL = computedTTL;
    strategy.lastAccessed = new Date();
    
    return Math.floor(computedTTL);
  }

  private getBaseTTL(type: string): number {
    const baseTTLs: Record<string, number> = {
      'legal': 7200,      // 2 hours
      'evidence': 3600,   // 1 hour  
      'search': 1800,     // 30 minutes
      'report': 7200,     // 2 hours
      'embedding': 86400, // 24 hours
      'default': 3600     // 1 hour
    };
    
    return baseTTLs[type] || baseTTLs.default;
  }

  private inferPriority(type: string): 'critical' | 'high' | 'medium' | 'low' {
    const priorityMap: Record<string, 'critical' | 'high' | 'medium' | 'low'> = {
      'legal': 'critical',
      'evidence': 'high', 
      'search': 'medium',
      'report': 'medium',
      'embedding': 'high'
    };
    
    return priorityMap[type] || 'medium';
  }

  private getPriorityMultiplier(priority: 'critical' | 'high' | 'medium' | 'low'): number {
    const multipliers = {
      'critical': 2.0,
      'high': 1.5,
      'medium': 1.0,
      'low': 0.7
    };
    
    return multipliers[priority];
  }

  /**
   * 3. CACHE HIT RATE MONITORING
   */
  private handleCacheHit(data: {key: string; latency: number; timestamp: number}) {
    this.updateMetrics('hit', data);
    
    // Update access frequency for TTL calculation
    const strategy = this.ttlStrategies.get(data.key);
    if (strategy) {
      strategy.accessFrequency += 1;
      strategy.lastAccessed = new Date();
    }
  }

  private handleCacheMiss(data: {key: string; query: string; timestamp: number}) {
    this.updateMetrics('miss', data);
    
    // Consider pre-loading similar queries
    this.scheduleProactiveLoad(data.query);
  }

  private updateMetrics(type: 'hit' | 'miss', data: any) {
    const totalRequests = this.metrics.hitRate + this.metrics.missRate + 1;
    
    if (type === 'hit') {
      this.metrics.hitRate = (this.metrics.hitRate * (totalRequests - 1) + 1) / totalRequests;
      this.metrics.averageLatency = (this.metrics.averageLatency * 0.9) + (data.latency * 0.1);
    } else {
      this.metrics.missRate = (this.metrics.missRate * (totalRequests - 1) + 1) / totalRequests;
    }
    
    // Emit metrics for monitoring
    this.emit('metrics_updated', this.metrics);
  }

  /**
   * 4. REQUEST BATCHING WITH GPU OPTIMIZATION
   */
  private handleGPUUtilization(data: {utilization: number; temperature: number; timestamp: number}) {
    this.metrics.gpuUtilization = data.utilization;
    
    // Adjust batch size based on GPU utilization
    if (data.utilization < this.config.performance.gpuUtilizationTarget - 0.1) {
      // GPU underutilized - increase batch size
      this.requestBatcher.increaseBatchSize();
    } else if (data.utilization > this.config.performance.gpuUtilizationTarget + 0.1) {
      // GPU overutilized - decrease batch size
      this.requestBatcher.decreaseBatchSize();
    }
    
    this.emit('gpu_utilization_updated', data);
  }

  /**
   * Start continuous cache optimization
   */
  private startCacheOptimization() {
    // Initial warmup
    setTimeout(() => this.warmCache(), 1000);
    
    // Schedule regular optimization cycles
    this.warmupTimer = setInterval(async () => {
      await this.optimizationCycle();
    }, 300000); // Every 5 minutes
    
    console.log('üîÑ Started continuous cache optimization');
  }

  private async optimizationCycle() {
    console.log('üîß Running cache optimization cycle...');
    
    try {
      // 1. Analyze current performance
      await this.analyzePerformance();
      
      // 2. Optimize TTL values based on patterns
      await this.optimizeTTLStrategies();
      
      // 3. Preemptively cache predicted queries
      await this.predictivePreload();
      
      // 4. Clean up stale entries
      await this.cleanupStaleEntries();
      
      this.metrics.lastOptimized = new Date();
      console.log('‚úÖ Cache optimization cycle completed');
      
    } catch (error) {
      console.error('‚ùå Cache optimization cycle failed:', error);
      this.emit('optimization_error', error);
    }
  }

  /**
   * Get current cache performance metrics
   */
  getMetrics(): CacheMetrics {
    return { ...this.metrics };
  }

  /**
   * Get TTL strategies for debugging
   */
  getTTLStrategies(): Map<string, TTLStrategy> {
    return new Map(this.ttlStrategies);
  }

  /**
   * Manual cache warming trigger
   */
  async triggerWarmup(): Promise<void> {
    await this.warmCache();
  }

  /**
   * Cleanup resources
   */
  async cleanup(): Promise<void> {
    if (this.warmupTimer) {
      clearInterval(this.warmupTimer);
      this.warmupTimer = null;
    }
    
    if (this.redis) {
      await this.redis.quit();
    }
    
    console.log('üßπ Cache optimizer cleaned up');
  }

  // Helper methods (implementation details)
  private hashQuery(query: string): string {
    // Simple hash function for cache keys
    let hash = 0;
    for (let i = 0; i < query.length; i++) {
      const char = query.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32bit integer
    }
    return hash.toString(16);
  }

  private async executeQueryForCache(query: string): Promise<any> {
    // Simulate query execution - replace with actual implementation
    return {
      query,
      results: [],
      timestamp: Date.now(),
      fromCache: false
    };
  }

  private async getRecentDocumentsByType(docType: string, limit: number): Promise<any[]> {
    // Simulate document retrieval - replace with actual implementation
    return Array.from({ length: Math.min(limit, 10) }, (_, i) => ({
      id: `${docType}_${i}`,
      type: docType,
      content: `Sample ${docType} content ${i}`
    }));
  }

  private async preloadUserPattern(pattern: string): Promise<void> {
    console.log(`üë§ Preloading user pattern: ${pattern}`);
    // Implementation for user pattern preloading
  }

  private async scheduleProactiveLoad(query: string): Promise<void> {
    // Schedule similar queries for preloading
    console.log(`üîç Scheduling proactive load for similar to: ${query}`);
  }

  private async analyzePerformance(): Promise<void> {
    // Analyze cache performance patterns
    console.log('üìä Analyzing cache performance...');
  }

  private async optimizeTTLStrategies(): Promise<void> {
    // Optimize TTL values based on access patterns
    console.log('‚è±Ô∏è Optimizing TTL strategies...');
  }

  private async predictivePreload(): Promise<void> {
    // Predictively preload likely queries
    console.log('üîÆ Running predictive preload...');
  }

  private async cleanupStaleEntries(): Promise<void> {
    // Clean up stale cache entries
    console.log('üßπ Cleaning up stale cache entries...');
  }
}

/**
 * Request Batcher for GPU Optimization
 */
class RequestBatcher {
  private batchSize: number;
  private maxConcurrency: number;
  private currentBatch: Array<() => Promise<any>>;
  private processing: boolean;

  constructor(config: {batchSize: number; maxConcurrency: number; gpuUtilizationTarget: number}) {
    this.batchSize = config.batchSize;
    this.maxConcurrency = config.maxConcurrency;
    this.currentBatch = [];
    this.processing = false;
  }

  async executeBatch(tasks: Array<() => Promise<any>>): Promise<{successful: number; total: number; errors: any[]}> {
    const results = {
      successful: 0,
      total: tasks.length,
      errors: [] as any[]
    };

    // Process tasks in batches
    for (let i = 0; i < tasks.length; i += this.batchSize) {
      const batch = tasks.slice(i, i + this.batchSize);
      
      try {
        const batchResults = await Promise.allSettled(
          batch.map(task => task())
        );
        
        batchResults.forEach(result => {
          if (result.status === 'fulfilled') {
            results.successful++;
          } else {
            results.errors.push(result.reason);
          }
        });
        
      } catch (error) {
        results.errors.push(error);
      }
    }

    return results;
  }

  increaseBatchSize(): void {
    this.batchSize = Math.min(this.batchSize * 1.2, 100);
    console.log(`üìà Increased batch size to ${Math.floor(this.batchSize)}`);
  }

  decreaseBatchSize(): void {
    this.batchSize = Math.max(this.batchSize * 0.8, 10);
    console.log(`üìâ Decreased batch size to ${Math.floor(this.batchSize)}`);
  }
}

export { type CacheWarmerConfig, type CacheMetrics, type TTLStrategy };