version: '3.8'

services:
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: legal-vllm-test
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./models:/app/models
      - ollama_data:/ollama_models:ro
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model /ollama_models/models/blobs/sha256-c6f6f9cd9fca55297e91ed31a52a4c9931e6396a504176b0c7a9390812dc8124
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.9
      --max-model-len 4096
      --enable-chunked-prefill
      --max-num-batched-tokens 8192
      --max-num-seqs 256
      --trust-remote-code
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Fallback test service without GPU requirements
  vllm-cpu-test:
    image: python:3.11-slim
    container_name: legal-vllm-cpu-test
    ports:
      - "8001:8000"
    volumes:
      - ollama_data:/ollama_models:ro
      - ./vllm-test:/app
    working_dir: /app
    command: >
      bash -c "
      pip install --no-cache-dir fastapi uvicorn torch transformers sentencepiece &&
      python cpu_vllm_mock.py
      "
    restart: unless-stopped

volumes:
  ollama_data:
    external: true

networks:
  default:
    name: legal-ai-network
    external: true