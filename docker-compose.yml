version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-app_db}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 1GB
      POSTGRES_WORK_MEM: 64MB
      POSTGRES_MAINTENANCE_WORK_MEM: 128MB
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_EXTENSIONS: vector
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-pgvector.sql:/docker-entrypoint-initdb.d/10-init-pgvector.sql
      - ./scripts/init-schema.sql:/docker-entrypoint-initdb.d/20-init-schema.sql
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-app_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - prosecutor_network

  qdrant:
    image: qdrant/qdrant:v1.9.2
    ports:
      - "${QDRANT_HTTP_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__STORAGE__ON_DISK_PAYLOAD: "true"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - prosecutor_network

  redis:
    image: redis:7-alpine
    container_name: prosecutor_redis
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru --appendonly yes
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - prosecutor_network

  ollama:
    image: ollama/ollama:latest
    container_name: prosecutor_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-models.txt:/tmp/models.txt
    # Pull models on startup
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "ollama serve &
      sleep 10 &&
      while IFS= read -r model || [ -n \"$$model\" ]; do
        echo \"Pulling model: $$model\"
        ollama pull \"$$model\" || echo \"Failed to pull $$model\"
      done < /tmp/models.txt 2>/dev/null || echo 'No models file found' &&
      wait"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - prosecutor_network
    # GPU support for WSL2 (if available)
    # Uncomment the following lines if you have NVIDIA GPU support in WSL2
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    depends_on:
      - postgres
    networks:
      - prosecutor_network

networks:
  prosecutor_network:
    driver: bridge

volumes:
  postgres_data:
  qdrant_data:
  redis_data:
  ollama_data:
  pgadmin_data:
